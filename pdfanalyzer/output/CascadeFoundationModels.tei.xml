<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-03">3 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,106.97,164.94,66.08,10.37;1,173.05,162.94,2.51,6.99"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
							<email>zhangrenrui@pjlab.org.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.58,164.94,59.43,10.37;1,254.01,162.94,2.51,6.99"><forename type="first">Xiangfei</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.54,164.94,44.83,10.37"><forename type="first">Bohao</forename><surname>Li</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.08,164.94,68.07,10.37"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
							<email>huangsiyuan@pjlab.org.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.45,164.94,64.08,10.37"><forename type="first">Hanqiu</forename><surname>Deng</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.67,179.00,68.07,10.37"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.45,179.00,39.51,10.37"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>qiaoyu@pjlab.org.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.67,179.00,46.82,10.37;1,376.49,177.00,2.92,6.99"><forename type="first">Peng</forename><surname>Gao</surname></persName>
							<email>gaopeng@pjlab.org.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-03">3 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">572D7A4BE47336ACC2517A3136E2E4F8</idno>
					<idno type="arXiv">arXiv:2303.02151v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-03-08T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual recognition in low-data regimes requires deep neural networks to learn generalized representations from limited training samples. Recently, CLIP-based methods have shown promising few-shot performance benefited from the contrastive language-image pre-training. We then question, if the more diverse pre-training knowledge can be cascaded to further assist few-shot representation learning. In this paper, we propose CaFo, a Cascade of Foundation models that incorporates diverse prior knowledge of various pretraining paradigms for better few-shot learning. Our CaFo incorporates CLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge, DALL-E's visiongenerative knowledge, and GPT-3's language-generative knowledge. Specifically, CaFo works by 'Prompt, Generate, then Cache'. Firstly, we leverage GPT-3 to produce textual inputs for prompting CLIP with rich downstream linguistic semantics. Then, we generate synthetic images via DALL-E to expand the few-shot training data without any manpower. At last, we introduce a learnable cache model to adaptively blend the predictions from CLIP and DINO. By such collaboration, CaFo can fully unleash the potential of different pre-training methods and unify them to perform state-ofthe-art for few-shot classification. Code is available at https://github.com/ZrrSkywalker/CaFo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks <ref type="bibr" coords="1,199.92,659.78,16.60,8.64" target="#b42">[42]</ref> and transformers <ref type="bibr" coords="1,64.73,671.73,16.60,8.64" target="#b67">[67]</ref> have attained great success on a wide range of vision tasks with abundant datasets <ref type="bibr" coords="1,189.86,683.69,15.27,8.64" target="#b14">[15]</ref>. Instead, for some * Equal contribution. ‚Ä† Corresponding author data-deficient and resource-finite scenarios, few-shot learning <ref type="bibr" coords="1,323.94,525.19,15.77,8.64" target="#b62">[62,</ref><ref type="bibr" coords="1,340.74,525.19,13.28,8.64" target="#b69">69]</ref> also becomes a research hotspot, where the networks are constrained to learn from limited images with annotations. Many previous works have been proposed in this field to enhance model's generalization capability by meta learning <ref type="bibr" coords="1,382.67,573.01,15.77,8.64" target="#b19">[20,</ref><ref type="bibr" coords="1,400.71,573.01,11.83,8.64" target="#b70">70]</ref>, metric learning <ref type="bibr" coords="1,486.78,573.01,15.27,8.64" target="#b73">[73]</ref>, and data augmentation <ref type="bibr" coords="1,367.73,584.96,15.77,8.64" target="#b28">[28,</ref><ref type="bibr" coords="1,386.23,584.96,11.83,8.64" target="#b72">72]</ref>. Recently, CLIP <ref type="bibr" coords="1,480.18,584.96,16.60,8.64" target="#b56">[56]</ref> pre-trained by large-scale language-image pairs shows favorable zeroshot transfer ability for open-vocabulary visual recognition. The follow-up CoOp <ref type="bibr" coords="1,425.36,620.83,15.27,8.64" target="#b81">[81]</ref>, CLIP-Adapter <ref type="bibr" coords="1,510.04,620.83,16.60,8.64" target="#b21">[22]</ref> and Tip-Adapter <ref type="bibr" coords="1,361.42,632.78,16.60,8.64" target="#b75">[75]</ref> further extend it for few-shot classification and achieve superior performance on various downstream datasets. This indicates that, even if the few-shot training data is insufficient, the large-scale pre-training has endowed the network with strong representation ability, which highly benefits the few-shot learning on downstream domains. Now that there exist various self-supervisory paradigms besides CLIP, could we adaptively integrate their pre-learned knowledge and collaborate them to be a better few-shot learner?</p><p>To tackle this issue, we propose CaFo, a Cascade of Fooundation models blending the knowledge from multiple pre-training paradigms with a 'Prompt, Generate, then Cache' pipeline. As shown in Figure <ref type="figure" coords="2,244.72,149.10,3.74,8.64" target="#fig_0">1</ref>, we integrate CLIP <ref type="bibr" coords="2,108.08,161.06,15.27,8.64" target="#b56">[56]</ref>, DINO <ref type="bibr" coords="2,162.04,161.06,10.58,8.64" target="#b6">[7]</ref>, DALL-E <ref type="bibr" coords="2,221.69,161.06,15.27,8.64" target="#b57">[57]</ref>, and GPT-3 <ref type="bibr" coords="2,58.82,173.01,11.62,8.64" target="#b3">[4]</ref> to provide four types of prior knowledge for CaFo. Therein, CLIP <ref type="bibr" coords="2,114.00,184.97,16.60,8.64" target="#b56">[56]</ref> is pre-trained to produce paired features in the embedding space for every image and its descriptive text. Guided by texts with different categorical semantics, CLIP <ref type="bibr" coords="2,143.28,220.83,16.60,8.64" target="#b56">[56]</ref> can well classify the images aided by language-contrastive knowledge. DINO follows contrastive self-supervised learning <ref type="bibr" coords="2,219.54,244.74,11.62,8.64" target="#b6">[7]</ref> to match the representations between two transformations of one same image, which is expert at distinguishing different images with vision-contrastive knowledge. Similar to CLIP <ref type="bibr" coords="2,267.27,280.61,15.27,8.64" target="#b56">[56]</ref>, DALL-E <ref type="bibr" coords="2,89.85,292.56,16.60,8.64" target="#b57">[57]</ref> is also pre-trained by image-text pairs but learns to predict the encoded image tokens based on the given text tokens. Conditioned on the input text, DALL-E <ref type="bibr" coords="2,59.14,328.43,16.60,8.64" target="#b57">[57]</ref> could leverage the vision-generative knowledge to create high-quality synthetic images in a zero-shot manner. Pre-trained by large-scale language corpus, GPT-3 <ref type="bibr" coords="2,252.11,352.34,11.62,8.64" target="#b3">[4]</ref> takes a few hand-written templates as input, and autoregressively generates human-like texts, which contain rich languagegenerative knowledge. Therefore, the four models have distinctive pre-training goals and can provide complementary knowledge to assist the few-shot visual recognition.</p><p>In detail, we cascade them by three steps.: 1) Prompt. We adopt GPT-3 <ref type="bibr" coords="2,121.68,437.92,11.62,8.64" target="#b3">[4]</ref> to produce textual prompts for CLIP based on a few hand-written templates. These prompts with richer language knowledge are fed into CLIP's textual encoder. 2) Generate. We adopt DALL-E <ref type="bibr" coords="2,228.34,473.78,16.60,8.64" target="#b57">[57]</ref> to generate additional training images for different categories based on the domain-specific texts, which enlarges the few-shot training data, but costs no extra manpower for collection and annotation. 3) Cache. We utilize a cache model to adaptively incorporate the predictions from both CLIP <ref type="bibr" coords="2,269.77,533.56,16.60,8.64" target="#b56">[56]</ref> and DINO <ref type="bibr" coords="2,97.60,545.51,10.58,8.64" target="#b6">[7]</ref>. Referring to Tip-Adapter <ref type="bibr" coords="2,226.77,545.51,15.27,8.64" target="#b75">[75]</ref>, we build the cache model with two kinds of keys respectively for the two pre-trained models. Regarding zero-shot CLIP as the distribution baseline, we adaptively ensemble the predictions of two cached keys as the final output. By only fine-tuning the lightweight cache model via expanded training data, CaFo can learn to fuse diverse prior knowledge and leverage their complementary characteristics for better few-shot visual recognition.</p><p>Our main contributions are summarized as follows:</p><p>‚Ä¢ We propose CaFo to incorporate the prior knowledge learned from various pre-training paradigms for better few-shot learning.</p><p>‚Ä¢ By collaborating CLIP, DINO, GPT-3 and DALL-E, CaFo utilizes more semantic prompts, enriches the limited few-shot training data, and adaptively ensembles diverse predictions via the cache model.</p><p>‚Ä¢ We conduct thorough experiments on 11 datasets for few-shot classification, where CaFo achieves state-ofthe-art without using extra annotated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Pre-training of Vision Models. With the breakthroughs in deep learning models <ref type="bibr" coords="2,411.51,211.02,15.77,8.64" target="#b17">[18,</ref><ref type="bibr" coords="2,429.35,211.02,12.45,8.64" target="#b33">33,</ref><ref type="bibr" coords="2,443.89,211.02,11.83,8.64" target="#b46">46]</ref>, most modern vision models are based on the paradigm of pre-training on Ima-geNet <ref type="bibr" coords="2,335.46,234.93,16.60,8.64" target="#b14">[15]</ref> and fine-tuning on downstream tasks <ref type="bibr" coords="2,505.37,234.93,15.27,8.64" target="#b32">[32]</ref>. Pretrained models have shown promising adaptability for various downstream tasks, such as object detection <ref type="bibr" coords="2,510.34,258.84,15.27,8.64" target="#b44">[44]</ref>, semantic segmentation <ref type="bibr" coords="2,396.20,270.79,10.58,8.64" target="#b7">[8]</ref>, and 3D recognition <ref type="bibr" coords="2,497.02,270.79,15.77,8.64" target="#b26">[26,</ref><ref type="bibr" coords="2,514.84,270.79,12.45,8.64" target="#b76">76,</ref><ref type="bibr" coords="2,529.34,270.79,11.83,8.64" target="#b79">79]</ref>.</p><p>To improve the representation capability by overcoming the constraints of annotation, self-supervised pre-training has attracted wide attention using large-scale unlabeled datasets <ref type="bibr" coords="2,308.86,318.62,15.27,8.64" target="#b40">[40]</ref>. Self-supervised learning is initialized by pretext tasks, such as image restoration from corruption <ref type="bibr" coords="2,494.83,330.57,15.77,8.64" target="#b30">[30,</ref><ref type="bibr" coords="2,513.74,330.57,12.45,8.64" target="#b54">54,</ref><ref type="bibr" coords="2,529.34,330.57,11.83,8.64" target="#b68">68]</ref>, pseudo labels <ref type="bibr" coords="2,365.26,342.53,15.77,8.64" target="#b16">[17,</ref><ref type="bibr" coords="2,382.29,342.53,13.28,8.64" target="#b52">52]</ref> and clustering <ref type="bibr" coords="2,456.65,342.53,10.58,8.64" target="#b4">[5]</ref>. Recently, contrast learning, which learns representations by contrasting positive pairs against negative pairs, has gotten well studied for diverse visual representation learning <ref type="bibr" coords="2,463.99,378.39,10.79,8.64" target="#b6">[7,</ref><ref type="bibr" coords="2,476.72,378.39,7.47,8.64" target="#b8">9,</ref><ref type="bibr" coords="2,486.15,378.39,12.45,8.64" target="#b9">10,</ref><ref type="bibr" coords="2,500.54,378.39,12.45,8.64" target="#b25">25,</ref><ref type="bibr" coords="2,514.94,378.39,12.45,8.64" target="#b31">31,</ref><ref type="bibr" coords="2,529.34,378.39,11.83,8.64" target="#b66">66]</ref>. Besides, language-supervised visual pre-training emerges as a novel paradigm closer to natural visual understanding <ref type="bibr" coords="2,324.57,414.26,10.79,8.64" target="#b1">[2,</ref><ref type="bibr" coords="2,336.99,414.26,12.45,8.64" target="#b49">49,</ref><ref type="bibr" coords="2,351.06,414.26,12.45,8.64" target="#b58">58,</ref><ref type="bibr" coords="2,365.13,414.26,11.83,8.64" target="#b61">61]</ref>, among which CLIP <ref type="bibr" coords="2,466.00,414.26,16.60,8.64" target="#b56">[56]</ref> obtains powerful zero-shot transferability by contrastive pre-training on image-text pairs from the Internet. In addition, visionlanguage pre-training can also promote the zero-shot image generation from text. Open generative models, such as DALL-E <ref type="bibr" coords="2,360.41,474.03,16.60,8.64" target="#b57">[57]</ref> and CogView <ref type="bibr" coords="2,440.27,474.03,16.60,8.64" target="#b15">[16]</ref> pre-trained on largescale image-text pairs are able to generate images with diverse contents by given texts. In this paper, CaFo cascade three visual pre-training models, CLIP, DINO, and DALL-E, which contributes to better few-shot learning capacity.</p><p>Language-assisted Vision Models. As different form of data, linguistic knowledge normally contains complementary knowledge to images. For vision-language models, several works <ref type="bibr" coords="2,365.34,584.96,12.73,8.64" target="#b3">[4,</ref><ref type="bibr" coords="2,378.07,584.96,12.73,8.64" target="#b23">23,</ref><ref type="bibr" coords="2,390.79,584.96,12.73,8.64" target="#b56">56]</ref> have showed the format of prompts would highly affect the accuracy on vision tasks. Thus, prompt engineering is worth putting in great effort. Some efforts <ref type="bibr" coords="2,336.68,620.83,17.13,8.64" target="#b37">[37,</ref><ref type="bibr" coords="2,353.81,620.83,12.85,8.64" target="#b59">59,</ref><ref type="bibr" coords="2,366.65,620.83,12.85,8.64" target="#b78">78,</ref><ref type="bibr" coords="2,379.50,620.83,12.85,8.64" target="#b82">82]</ref> utilize learnable textual inputs and optimize them during training. Other works <ref type="bibr" coords="2,479.86,632.78,15.77,8.64" target="#b48">[48,</ref><ref type="bibr" coords="2,497.27,632.78,13.28,8.64" target="#b55">55]</ref> propose to leverage linguistic knowledge pre-trained from large language models to generate prompts for each visual category, which enhances vision-language models without any additional training or labeling. Our CaFo refers to CuPL <ref type="bibr" coords="2,518.36,680.60,16.60,8.64" target="#b55">[55]</ref> to produce semantic-rich texts to prompt CLIP for better textimage alignment.</p><p>Few-shot Learning. Few-shot learning highly relies on the transferability of the trained neural networks. From the perspective of distance measurement, some metric learning methods learn a metric space by computing the distances from the instances to novel categories <ref type="bibr" coords="3,210.16,123.30,15.77,8.64" target="#b62">[62,</ref><ref type="bibr" coords="3,228.03,123.30,12.45,8.64" target="#b64">64,</ref><ref type="bibr" coords="3,242.58,123.30,11.83,8.64" target="#b69">69]</ref>. Also, meta-learning is proposed to improve the few-shot adaptation ability of the models by finding a set of initialized parameters that can rapidly adapt to novel domains <ref type="bibr" coords="3,244.02,159.16,16.94,8.64" target="#b11">[12,</ref><ref type="bibr" coords="3,260.96,159.16,12.70,8.64" target="#b20">21,</ref><ref type="bibr" coords="3,273.66,159.16,12.70,8.64" target="#b38">38,</ref><ref type="bibr" coords="3,50.11,171.12,11.83,8.64" target="#b43">43]</ref>. More recently, with the vision-language pre-training model CLIP <ref type="bibr" coords="3,100.48,183.07,16.60,8.64" target="#b56">[56]</ref> exhibiting strong zero-shot adaptation performance, several efforts have started to find efficient strategies to adapt it to downstream few-shot datasets. CoOp <ref type="bibr" coords="3,269.77,206.98,16.60,8.64" target="#b81">[81]</ref> is proposed as a prompt tuning adaptation method by optimizing a set of learnable prompt tokens. Subsequently, to inject textual branch with visual signals, CoCoOp <ref type="bibr" coords="3,269.77,242.85,16.60,8.64" target="#b83">[83]</ref> and VT-CLIP <ref type="bibr" coords="3,107.69,254.80,16.60,8.64" target="#b78">[78]</ref> propose to train a intermediate network to generate image tokens as conditional inputs for the textual vectors. Referring to adapters <ref type="bibr" coords="3,204.47,278.72,16.60,8.64" target="#b36">[36]</ref> in natural language processing, CLIP-Adapter <ref type="bibr" coords="3,183.95,290.67,16.60,8.64" target="#b21">[22]</ref> is introduced to finetune CLIP by applying lightweight residual-style adapters. Tip-Adapter <ref type="bibr" coords="3,102.24,314.58,16.60,8.64" target="#b75">[75]</ref> is then proposed as a training-free adaption method with a constructed key-value cache model. It can also be regarded as a better initialization of CLIP-Adapter with much faster convergence when fine-tuning. CALIP <ref type="bibr" coords="3,81.37,362.40,16.60,8.64" target="#b27">[27]</ref> proposes a parameter-free attention to enhance CLIP in a zero-shot manner, and its parametric solution further attains higher few-shot accuracy. SuS-X <ref type="bibr" coords="3,248.79,386.31,16.60,8.64" target="#b65">[65]</ref> constructs a dynamic support set and extends Tip-Adapter by leveraging image-text distances. Besides, many follow-up works <ref type="bibr" coords="3,78.03,422.18,15.77,8.64" target="#b37">[37,</ref><ref type="bibr" coords="3,95.88,422.18,12.45,8.64" target="#b45">45,</ref><ref type="bibr" coords="3,110.42,422.18,12.45,8.64" target="#b65">65,</ref><ref type="bibr" coords="3,124.95,422.18,12.45,8.64" target="#b74">74,</ref><ref type="bibr" coords="3,139.49,422.18,12.45,8.64" target="#b77">77,</ref><ref type="bibr" coords="3,154.02,422.18,12.45,8.64" target="#b80">80,</ref><ref type="bibr" coords="3,168.56,422.18,13.28,8.64" target="#b84">84]</ref> have also been proposed for further adapting CLIP to various vision tasks. Different from all existing methods, we integrate other powerful pretraining paradigms with CLIP and collaborate them with customized pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cascade of Foundation Models</head><p>In this section, we first briefly revisit four types of pretraining paradigms in CaFo. Then, we specifically introduce how we cascade them by 'Prompt, Generate, then Cache'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Different Pre-training Paradigms</head><p>Contrastive Vision-Language Pre-training. The series <ref type="bibr" coords="3,69.32,596.92,11.62,8.64" target="#b8">[9]</ref> of contrastive learning between vision and language learn to map the two modalities into the same embedding space via a contrastive loss. Driven by web-scale datasets, e.g., 400 million for CLIP <ref type="bibr" coords="3,194.40,632.78,16.60,8.64" target="#b56">[56]</ref> and 1.8 billion for ALIGN <ref type="bibr" coords="3,83.29,644.74,15.27,8.64" target="#b39">[39]</ref>, the basic pre-training target is to minimize the embedding distances of images and their textual descriptions, while maximize those unpaired ones. By the crossmodal alignment, we can discriminate images of different categorizes by the texts with different semantics. We denote such learned prior as language-contrastive knowledge Contrastive Vision Pre-training. As the traditional selfsupervised learning methods, vision-contrastive models <ref type="bibr" coords="3,533.50,311.42,11.62,8.64" target="#b8">[9]</ref> focus on the discrimination between different images. Normally, the positive pairs to be drawn close are two transformations of the same image, while the optimization of negative pairs <ref type="bibr" coords="3,368.88,359.24,16.60,8.64" target="#b24">[24]</ref> is optional, which can be replaced by a momentum encoder <ref type="bibr" coords="3,399.39,371.20,16.60,8.64" target="#b31">[31]</ref> or cluster assignments <ref type="bibr" coords="3,511.81,371.20,10.58,8.64" target="#b5">[6]</ref>. Recent works reveal that we can learn self-supervised features without negative pairs between images <ref type="bibr" coords="3,489.21,395.11,10.79,8.64" target="#b6">[7,</ref><ref type="bibr" coords="3,501.48,395.11,11.83,8.64" target="#b25">25]</ref>. Given the strong linear classification capacity, the pre-trained DINO <ref type="bibr" coords="3,338.81,419.02,11.62,8.64" target="#b6">[7]</ref> is adopted here to provide vision-contrastive knowledge for collaboration.</p><p>Generative Language Pre-training. With 175 billion parameters, the large-scale pre-trained GPT-3 <ref type="bibr" coords="3,499.14,466.13,11.62,8.64" target="#b3">[4]</ref> is powerful to produce human-like texts with diverse contents and incredible quality. Taking as input a few designed language commands, GPT-3 is able to output prompts with rich linguistic semantics for vision-language models. CLIP utilizes handcrafted templates as prompts, e.g., "a photo of a [CLASS]", which however lacks sufficient textual semantics to align with input images. We thus leverage GPT-3 to produce CLIP's prompts to better align with visual information from images.</p><p>Generative Vision-Language Pre-training. Learned from millions of image-caption pairs, the DALL-E series can generate language-conditioned images in a zero-shot manner. They are pre-trained to autoregressively predict the encoded image tokens from the textual tokens of the captions. With such language-generative knowledge, the pre-trained DALL-E can be viewed as a free lunch to enlarge the training data without any manpower. Considering publicity, we select DALL-E-mini <ref type="bibr" coords="3,446.18,692.56,16.60,8.64" target="#b13">[14]</ref> as the representative among DALL-E models. , then Cache by CLIP <ref type="bibr" coords="4,280.33,261.91,14.94,8.06" target="#b56">[56]</ref> and DINO <ref type="bibr" coords="4,341.78,261.91,9.52,8.06" target="#b6">[7]</ref>. We adopt DALL-E to generate synthetic images to expand the limited few-shot training samples. Then, we construct the cache model with two kinds of keys to adaptively fuse the knowledge from CLIP and DINO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Prompt, Generate, then Cache</head><p>To cascade different pre-training paradigms, we introduce CaFo with a pipeline of 'Prompt, Generate, then Cache', which respectively unleashes the powers of different self-supervised knowledge.</p><p>Prompt with GPT-3. Under the N -way K-shot settings, we have the few-shot training images I N,K with labels L N,K that contain K samples for each N categories. As shown in Figure <ref type="figure" coords="4,117.36,436.95,3.74,8.64" target="#fig_1">2</ref>, for N categories, we adopt a unified series of templates as the language command for GPT-3 <ref type="bibr" coords="4,272.26,448.90,10.58,8.64" target="#b3">[4]</ref>, e.g., "What a [CLASS] looks like?", "How can you identify a [CLASS]?", and "A caption of an image of a [CLASS]:". We denote the created prompts for N categories as P N , formulated as</p><formula xml:id="formula_0" coords="4,113.57,519.57,172.80,9.65">P N = GPT-3(Commands).<label>(1)</label></formula><p>Then, we adopt P N as the input of CLIP's textual encoder.</p><p>Further, for some downstream data with specialized categories, we can customize the language commands for producing prompts with more domain-specific semantics. For example, in OxfordPets <ref type="bibr" coords="4,145.31,590.87,16.60,8.64" target="#b53">[53]</ref> dataset of pet images, we adopt the input of GPT-3 as "This is a pet bulldog, it has thin neck, short face, floppy ears. It's coat is short, straight, and in brindle color. This is a pet [CLASS],". Based on that, GPT-3 continues to describe more details of the [CLASS] pet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generate via DALL-E Via the zero-shot DALL-E [14],</head><p>we generate synthesis images to enrich our limited training images I N,K , as shown in Figure <ref type="figure" coords="4,206.74,692.56,4.98,8.64" target="#fig_2">3</ref> (1). For different categories, we adopt a simple template, e.g., "a photo of </p><formula xml:id="formula_1" coords="4,346.55,373.13,198.56,9.96">I N,(K+K ) = {DALL-E(T N ), I N,K }, ,<label>(2)</label></formula><p>where T N denotes the N -category textual inputs. We keep K comparable with K to ensure the synthesis quality and also preserve the low-data regimes. By the pre-trained language-generative knowledge, the data expansion is totally zero-shot, which requires no manpower to collect or annotate the data, and alleviates the data deficiency issue inherently for few-shot learning.</p><p>Cache by CLIP and DINO. We construct a key-value cache model for adaptive knowledge ensemble. Different from Tip-Adapter <ref type="bibr" coords="4,381.01,516.31,16.60,8.64" target="#b75">[75]</ref> only adapting CLIP, our cache model contains the pre-learned knowledge from both CLIP and DINO by caching two kinds of keys. Specifically in Figure <ref type="figure" coords="4,324.31,552.17,4.98,8.64" target="#fig_4">4</ref> (2), we first utilize CLIP and DINO to independently extract visual features of the few-shot training images, formulated as</p><formula xml:id="formula_2" coords="4,367.15,596.64,177.96,9.96">F CLIP = CLIP vis (I N,(K+K ) );<label>(3)</label></formula><formula xml:id="formula_3" coords="4,364.83,611.59,180.28,9.96">F DINO = DINO(I N,(K+K ) ),<label>(4)</label></formula><p>where CLIP vis denotes the CLIP's visual encoder and  </p><formula xml:id="formula_4" coords="4,308.86,642.85,125.76,11.38">F CLIP , F DINO ‚àà R N (K+K )√óC .</formula><formula xml:id="formula_5" coords="5,178.90,72.72,352.20,144.78">ùë≥ 1,4 ùúë ‚Ä¢ ùëì !"#$ ùúë ‚Ä¢ ùëì %#&amp;' ùëù !"#$ ùëù %#&amp;' ùëù () reweight reweight CLIP ùë§ !"#$ ùë§ %#&amp;' ùê∏ùëõùë†ùëíùëöùëèùëôùëíùëë ùëôùëúùëîùëñùë°ùë† Cache Model</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaptive Inference</head><p>For a test image in Figure <ref type="figure" coords="5,178.29,292.58,3.74,8.64" target="#fig_4">4</ref>, we first extract its two visual features f CLIP , f DINO ‚àà R 1√óC and regard them as queries to retrieve diverse knowledge from the cache model. Then, we could acquire three predicted classification logits p ZS , p CLIP , p DINO ‚àà R 1√óN , which are respectively from CLIP's zero-shot alignment and the two keys of cache model. We formulate them as</p><formula xml:id="formula_6" coords="5,110.33,382.24,176.04,11.88">p ZS = f CLIP CLIP tex (P N ) T ;<label>(5)</label></formula><formula xml:id="formula_7" coords="5,103.35,398.69,183.01,12.85">p CLIP = œï(f CLIP F T CLIP ) L onehot ;<label>(6)</label></formula><formula xml:id="formula_8" coords="5,101.04,415.14,185.33,12.85">p DINO = œï(f DINO F T DINO ) L onehot ,<label>(7)</label></formula><p>where CLIP tex represents CLIP's textual encoder, P N denotes GPT-3's created prompts, and f CLIP F T CLIP denotes the query-key affinity matrix of the CLIP's keys, analogous to DINO's. œï(x) = exp(‚àíŒ≤ ‚Ä¢ (1 ‚àí x)) serves as a non-linear modulator to control the sharpness of affinity matrix.</p><p>As the language-contrastive p ZS is pre-trained by 400 million data and can perform strong zero-shot transfer ability, we regard p ZS as the prediction baseline and calculate the weights of p CLIP , p DINO for ensemble based on their distribution similarity with p ZS . By this, we can suppress some obviously false category possibilities in p CLIP , p DINO and also amplify the moderately correct ones during ensemble. Firstly, we respectively normalize the scales of three classification logits into -1‚àº1 by their each mean and standard deviation. We then calculate the distribution similarities as the ensemble weights for the two logits of the cache as</p><formula xml:id="formula_9" coords="5,87.05,635.11,199.32,12.85">w CLIP = p CLIP p T ZS ; w DINO = p DINO p T ZS .<label>(8)</label></formula><p>Finally, we adopt the softmax function to normalize the weights and obtain the final ensemble logits as</p><formula xml:id="formula_10" coords="5,99.81,691.44,186.55,19.91">p en = p ZS + i p i ‚Ä¢ softmax(w i ),<label>(9)</label></formula><p>where i ‚àà {CLIP, DINO}. By such similarity-based ensemble, p en can adaptively fuse the prior knowledge learned by CLIP and DINO's pre-training and achieve stronger fewshot image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Datasets. We conduct few-shot experiments on 11 publicly available datasets: ImageNet <ref type="bibr" coords="5,473.99,388.39,15.27,8.64" target="#b14">[15]</ref>, Standford-Cars <ref type="bibr" coords="5,329.26,400.34,15.27,8.64" target="#b41">[41]</ref>, UCF101 <ref type="bibr" coords="5,387.00,400.34,15.27,8.64" target="#b63">[63]</ref>, Caltech101 <ref type="bibr" coords="5,455.80,400.34,15.27,8.64" target="#b18">[19]</ref>, Flowers102 <ref type="bibr" coords="5,526.03,400.34,15.27,8.64" target="#b51">[51]</ref>, SUN397 <ref type="bibr" coords="5,350.47,412.30,15.27,8.64" target="#b71">[71]</ref>, DTD <ref type="bibr" coords="5,404.56,412.30,15.27,8.64" target="#b12">[13]</ref>, EuroSAT <ref type="bibr" coords="5,475.26,412.30,15.27,8.64" target="#b34">[34]</ref>, FGVCAircraft <ref type="bibr" coords="5,330.69,424.25,15.27,8.64" target="#b47">[47]</ref>, OxfordPets <ref type="bibr" coords="5,402.57,424.25,15.27,8.64" target="#b53">[53]</ref>, and Food101 <ref type="bibr" coords="5,482.31,424.25,10.58,8.64" target="#b2">[3]</ref>. We follow Tip-Adapter <ref type="bibr" coords="5,360.33,436.21,16.60,8.64" target="#b75">[75]</ref> to train CaFo with 1, 2, 4, 8, 16 shots and test on the full test set. As we adopt DALL-E to generate training images in a zero-shot manner, we can train CaFo only by the generated images and report its zero-shot performance without few-shot training set.</p><p>Implementation. Our CaFo integrates the knowledge from pre-trained CLIP <ref type="bibr" coords="5,402.55,513.23,15.27,8.64" target="#b56">[56]</ref>, DINO <ref type="bibr" coords="5,452.68,513.23,10.58,8.64" target="#b6">[7]</ref>, DALL-E <ref type="bibr" coords="5,508.50,513.23,15.27,8.64" target="#b57">[57]</ref>, and GPT-3 <ref type="bibr" coords="5,338.09,525.19,10.58,8.64" target="#b3">[4]</ref>. For CLIP, we utilize ResNet-50 <ref type="bibr" coords="5,487.88,525.19,16.60,8.64" target="#b33">[33]</ref> as the visual encoder and its aligned transformer as the textual encoder. To align with the visual representation from CLIP, we also adopt DINO pre-trained upon ResNet-50. For DALL-E, we adopt different domain-specific textual templates as the input for different datasets, which correspond to the original textual prompts for CLIP's textual encoder.</p><p>For GPT-3, we adopt five simple templates as the language commands shared by different categories. Each command outputs ten prompts, which obtains fifty prompts in total.</p><p>For each category, we simply ensemble the features of different prompts following CuPL <ref type="bibr" coords="5,442.15,656.69,10.58,8.64" target="#b3">[4]</ref>. During training, we only set the two kinds of keys in cache model to be learnable and utilize the data augmentation following Tip-Adapter-F. We train CaFo using batch size 64 only for 20 epochs, and adopt AdamW optimizer with the initial learning rate 0.0001 with a cosine scheduler. Note that, we tune the hyperparameters in CaFo by the official validation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance</head><p>On ImageNet. We compare CaFo with other CLIPbased adaption methods on the most representative Im-ageNet <ref type="bibr" coords="6,85.34,549.10,15.49,8.64" target="#b14">[15]</ref>: CALIP <ref type="bibr" coords="6,152.63,549.10,15.27,8.64" target="#b27">[27]</ref>, Linear-probe CLIP <ref type="bibr" coords="6,267.28,549.10,15.27,8.64" target="#b56">[56]</ref>, CoOp <ref type="bibr" coords="6,78.08,561.05,15.27,8.64" target="#b81">[81]</ref>, CLIP-Adapter <ref type="bibr" coords="6,162.91,561.05,15.27,8.64" target="#b21">[22]</ref>, Tip-Adapter-F <ref type="bibr" coords="6,248.31,561.05,15.27,8.64" target="#b75">[75]</ref>, and CALIP-FS <ref type="bibr" coords="6,96.78,573.01,15.27,8.64" target="#b27">[27]</ref>. All these methods are based on the pretrained CLIP <ref type="bibr" coords="6,105.49,584.96,16.60,8.64" target="#b56">[56]</ref> with ResNet-50 visual encoders. As reported in Figure <ref type="figure" coords="6,120.43,596.92,4.98,8.64">5</ref> and  Distribution Shift. We further evaluate the robustness of CaFo to distribution shift by training on "Source" dataset and testing on "Target" datasets. In Table <ref type="table" coords="6,439.69,639.41,3.36,7.77" target="#tab_4">3</ref>, we select the "Source" as ImageNet and the "Target" as ImageNet-V2 <ref type="bibr" coords="6,473.01,650.37,14.94,7.77" target="#b60">[60]</ref> and ImageNet-Sketch <ref type="bibr" coords="6,336.05,661.33,13.74,7.77" target="#b35">[35]</ref>. As we can utilize some prior knowledge of the target domain for GPT-3 and DALL-E for prompting and generation, CaFo achieves the best out-of-distribution performance on the two "Target" datasets, surpassing the second-best Tip-Adapter-F by +3.28%, +0.88%, and +3.43%, respectively.  egory of different shots on ImageNet. We observe that the larger K does not lead to better few-shot performance. As we adopt pre-trained CLIP to select the top-K generated images, which are scored by the similarities between CLIP-encoded images and category texts, the larger K would contain more low-quality images and adversely affect the cache model. Furthermore, the amount of expanded data is comparable to the original K shots and thus preserves the characteristic of few-shot learning.</p><p>Adaptive Inference. In Table <ref type="table" coords="7,433.13,606.53,3.36,7.77" target="#tab_6">5</ref>, we ablate different ensemble methods of CLIP and DINO's predictions during inference on Im-ageNet. The first two rows represent the cache model with one type of keys respectively for two pre-trained models without ensemble. Then, we adopt average and maximum pooling between the two predictions and ensemble the result with pZS. However, such naive integration without adaptive weights causes accuracy degradation. In the last three rows, we calculate the distribution similarities for adaptive ensemble and respectively select the three logits as the baseline. As shown, using pZS as the distribution base- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization</head><p>DALL-E's Generated Images. In Figure <ref type="figure" coords="8,220.29,650.37,3.36,7.77">7</ref>, we visualize the synthetic images generated by DALL-E on ImageNet <ref type="bibr" coords="8,251.60,661.33,13.74,7.77" target="#b14">[15]</ref>, Ox-fordPets <ref type="bibr" coords="8,82.52,672.29,14.94,7.77" target="#b53">[53]</ref> and Caltech101 <ref type="bibr" coords="8,158.79,672.29,13.74,7.77" target="#b18">[19]</ref>. As shown, benefited from the vision-generative knowledge, the generated images can well highlight the downstream semantics of target category and effectively expand the few-shot training set in low-data regimes.   GPT-3's Prompts for CLIP. In Figure <ref type="figure" coords="8,469.10,529.89,3.36,7.77" target="#fig_7">8</ref>, We present a rectified example in ImageNet <ref type="bibr" coords="8,402.66,540.85,14.94,7.77" target="#b14">[15]</ref> aided by GPT-3's prompts in CaFo. As shown, prompting by GPT-3 (Left) produces more semantic texts compared to CLIP's handcrafted templates(Right), and better depicts the visual appearances in the image, which predicts the correct category of goldfish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose CaFo, a cascade of foundation models that comprehends diverse knowledge from different pre-training and follows the 'Prompt, Generate, then Cache' pipeline. We first incorporate the generative language model, GPT-3, for prompting CLIP with more semantic texts, and adopt DALL-E to expand the few-shot training data. Then, we adaptively fuse the visioncontrastive DINO with CLIP via a unified cache model. By collaboration, CaFo achieves state-of-the-art performance for few-shot learning on 11 datasets. Although CaFo has unified four types of pre-training, our future direction will focus on integrating more existing pre-trained knowledge, such as the masked-generative MAE <ref type="bibr" coords="9,72.76,109.00,13.74,7.77" target="#b30">[30]</ref>, the 3D-contrastive CrossPoint <ref type="bibr" coords="9,204.22,109.00,9.52,7.77" target="#b0">[1]</ref>, and 3D-generative I2P-MAE <ref type="bibr" coords="9,87.72,119.96,13.74,7.77" target="#b79">[79]</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLIP's top prediction: coucal</head><p>Overall score:</p><p>...is a crow-like bird with a long tail and a loud call.</p><p>...is a bird with a long tail and a dark brown plumage.</p><p>... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLIP's top prediction: coucal</head><p>Overall score:</p><p>...is a crow-like bird with a long tail and a loud call.</p><p>...is a bird with a long tail and a dark brown plumage.</p><p>... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLIP's top prediction: coucal</head><p>Overall score:</p><p>...is a crow-like bird with a long tail and a loud call.</p><p>...is a bird with a long tail and a dark brown plumage.</p><p>... -Score: 26.17 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLIP's top prediction: coucal</head><p>Overall score:</p><p>...is a crow-like bird with a long tail and a loud call.</p><p>...is a bird with a long tail and a dark brown plumage.</p><p>...   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLIP's top prediction: coucal</head><p>Overall score:</p><p>...is a crow-like bird with a long tail and a loud call.</p><p>...is a bird with a long tail and a dark brown plumage.</p><p>... ...is a crow-like bird with a long tail and a loud call.</p><p>...is a bird with a long tail and a dark brown plumage.</p><p>... ...is a crow-like bird with a long tail and a loud call.</p><p>...is a bird with a long tail and a dark brown plumage.</p><p>... ...is a crow-like bird with a long tail and a loud call.</p><p>...is a bird with a long tail and a dark brown plumage.</p><p>...   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,308.86,453.92,236.25,8.12;1,308.86,465.23,236.25,7.77;1,308.86,476.19,120.18,7.77"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The Cascade Paradigm of CaFo. We adaptively incorporate the knowledge from four types of pre-training methods and achieve a strong few-shot learner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,308.86,219.79,236.25,8.12;3,308.86,231.10,236.25,7.77;3,308.86,242.06,145.80,7.77"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Prompt with GPT-3<ref type="bibr" coords="3,424.53,219.79,9.52,8.06" target="#b3">[4]</ref>. As the first step in CaFo, we utilize the pre-trained GPT-3 to produce prompts with rich linguistic semantics for CLIP's textual encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,50.11,261.91,495.00,8.12;4,50.11,273.22,495.00,7.77;4,50.11,284.18,81.18,7.77"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. Generate via DALL-E<ref type="bibr" coords="4,176.85,261.91,13.74,8.06" target="#b57">[57]</ref>, then Cache by CLIP<ref type="bibr" coords="4,280.33,261.91,14.94,8.06" target="#b56">[56]</ref> and DINO<ref type="bibr" coords="4,341.78,261.91,9.52,8.06" target="#b6">[7]</ref>. We adopt DALL-E to generate synthetic images to expand the limited few-shot training samples. Then, we construct the cache model with two kinds of keys to adaptively fuse the knowledge from CLIP and DINO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,308.86,316.71,236.25,8.64;4,308.86,328.66,236.25,8.64;4,308.86,340.30,236.25,8.96;4,308.86,352.26,231.82,8.96"><head></head><label></label><figDesc>a [CLASS].". After the generation, we utilize CLIP to filter the top-K best-quality images as the newly-expanded training samples for each category. Then, we obtain the Ncategory (K + K )-sample training images, formulated as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,50.11,227.82,495.00,8.12;5,50.11,239.13,495.00,7.77;5,50.11,250.09,68.37,7.77"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Adaptive Inference with Cache Model. We regard the test image as a query and retrieves CLIP and DINO's knowledge from the corresponding two keys in the cache model. Then, we calculate the distribution similarities between different classification logits for adaptive ensemble.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,422.31,13.89,78.17,6.77;8,422.31,25.10,55.08,6.77;8,322.92,167.24,94.51,7.59;8,325.06,199.86,89.28,6.77;8,325.06,208.25,50.69,6.77;8,325.06,216.63,90.40,6.77;8,325.06,225.01,71.79,6.77;8,325.06,233.39,69.02,6.77;8,325.06,241.77,5.24,6.77;8,325.06,177.71,48.96,5.21;8,436.53,199.86,86.50,6.77;8,436.53,208.25,56.25,6.77;8,436.53,216.63,57.59,6.77;8,436.53,225.01,80.27,6.77;8,436.53,233.39,81.48,6.77;8,436.53,241.77,5.24,6.77;8,436.53,178.25,48.87,5.21;8,322.88,265.45,110.86,7.59;8,324.81,282.87,37.65,6.77;8,325.10,297.22,75.22,6.77;8,325.10,305.60,60.11,6.77;8,325.10,313.98,83.37,6.77;8,325.10,322.36,82.43,6.77;8,325.10,330.74,39.58,6.77;8,325.10,339.12,5.24,6.77;8,325.02,275.92,48.95,5.21;8,436.53,297.22,90.14,6.77;8,436.53,305.60,59.88,6.77;8,436.53,313.98,61.23,6.77;8,436.53,322.36,83.91,6.77;8,436.53,330.74,85.12,6.77;8,436.53,339.12,5.24,6.77;8,436.53,276.46,48.87,5.21;8,436.53,112.88,43.95,12.19;8,436.53,127.94,11.17,12.19;8,436.53,143.00,54.71,12.19;8,436.59,282.87,37.65,6.77;8,325.12,185.61,37.65,6.77;8,436.55,186.01,37.65,6.77"><head></head><label></label><figDesc>Our top prediction: goldfish Overall score:25.34 Our top pr ediction: goldfish ...has a shiny, orange-gold body with dark spotss... ...usually orange, red, or yellow, and by its shape, which is typically oval or round... ... -With GPT-3 prompts: a photo of the small [goldfish]. art of the [goldfish]. a origami [goldfish]. a [goldfish] in a video game. a bad photo of the [goldfish]. ... -With CLIP templates: CLIP's top pr ediction: cor al r eef -Score: 24.53 ...are composed of calcium carbonate skeletons... ...a large underwater structure made up of many small stony coral polyps... ... -With GPT-3 prompts: a photo of the small [coral reef]. art of the [coral reef]. a origami [coral reef]. a [coral reef] in a video game. a bad photo of the [coral reef].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,308.86,362.89,236.25,8.12;8,308.86,374.20,155.41,7.77"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Visualization of GPT-3's Prompts for CLIP. The example shown is from the ImageNet dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="14,50.11,651.80,236.25,8.12;14,50.11,663.11,129.60,7.77;14,169.63,555.06,81.26,78.64"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>Figure 10. t-SNE Visualization. Different colors represent different categories on 16-shot ImageNet.</figDesc><graphic coords="14,169.63,555.06,81.26,78.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="15,459.26,415.98,52.21,6.37;15,459.26,423.87,55.88,6.37;15,459.26,431.77,48.53,6.37;15,459.26,439.66,47.48,6.37;15,459.26,447.55,55.88,6.37;15,459.26,455.44,4.94,6.37;15,459.26,396.73,46.01,4.90;15,164.33,481.33,90.54,7.18;15,165.90,496.73,35.58,6.40;15,165.50,508.85,82.82,6.40;15,165.50,516.77,66.72,6.40;15,165.50,524.69,83.75,6.40;15,165.50,532.62,84.47,6.40;15,165.50,540.54,23.64,6.40;15,165.50,548.46,4.96,6.40;15,53.98,583.56,81.87,6.40;15,53.98,591.48,74.51,6.40;15,53.98,599.41,19.57,6.40;15,254.08,498.17,11.44,4.92;15,165.50,489.53,46.27,4.92"><head>a 33 -</head><label>33</label><figDesc>photo of the small [hammerhead shark]. art of the [coucal]. a bad photo of the [hammerhead shark]. ... -With CLIP templates: Our top prediction: stingray -Score: 29.20 ...has a flat body and a long tail with a stinger on the end. ...is a large, flat fish with a long tail that has a sharp spine on the end of it. ... ...are small, domesticated birds that are typically considered female.24.With GPT-3 prompts:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="15,309.27,615.03,11.44,4.92;15,307.45,634.65,11.44,4.92;15,307.15,654.26,11.44,4.92;15,220.48,608.04,55.59,4.92;15,152.45,615.74,38.13,18.22;15,356.54,629.80,38.13,18.22;15,268.20,496.77,35.58,6.40;15,267.80,508.89,52.41,6.40;15,267.80,516.82,27.32,6.40;15,267.80,524.74,41.05,6.40;15,267.80,532.66,42.32,6.40;15,267.80,540.58,76.63,6.40;15,267.80,548.50,4.96,6.40;15,267.80,489.57,46.18,4.92;15,356.56,481.28,110.58,7.18;15,358.13,496.69,35.58,6.40;15,363.32,508.81,82.78,6.40;15,363.32,516.73,63.58,6.40;15,363.32,524.65,70.78,6.40;15,363.32,532.57,74.05,6.40;15,363.32,540.49,29.86,6.40;15,363.32,548.42,4.96,6.40;15,446.31,498.13,11.44,4.92;15,357.73,489.49,46.27,4.92;15,461.19,496.73,35.58,6.40;15,460.79,508.85,75.62,6.40;15,460.79,516.77,12.28,6.40;15,460.79,524.69,60.96,6.40;15,460.79,532.62,70.87,6.40;15,460.79,540.54,12.28,6.40;15,460.79,548.46,4.96,6.40;15,460.79,489.53,46.18,4.92"><head>33 -</head><label>33</label><figDesc>the small [stingray]. art of the [hen]. a origami [hen]. a bad photo of the [stingray]. ... -With CLIP templates: CLIP's top prediction: electriv ray -Score: 29.03 ...is a flat fish that can deliver a powerful electric shock. ...is a flat, disk-shaped fish that can grow up to two feet in length„ÄÇ ...24.With GPT-3 prompts: -Score: 27.05 a photo of the small [electric ray]. art of the [electric ray]. a bad photo of the [electric ray]. ... -With CLIP templates:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="15,85.55,578.41,424.13,8.12;15,50.72,476.08,108.68,75.43"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Additional Visualization of GPT-3's Prompts for CLIP. Above examples are from the ImageNet dataset.</figDesc><graphic coords="15,50.72,476.08,108.68,75.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="16,308.24,328.80,11.42,4.91;16,306.42,348.38,11.42,4.91;16,306.12,367.97,11.42,4.91;16,219.60,321.82,55.50,4.91;16,151.67,329.50,38.07,18.19;16,355.44,343.54,38.07,18.19;16,267.23,210.73,35.53,6.39;16,266.84,222.83,69.53,6.39;16,266.84,230.74,40.98,6.39;16,266.84,238.65,42.25,6.39;16,266.84,246.56,63.65,6.39;16,266.84,254.46,64.79,6.39;16,266.84,262.37,4.95,6.39;16,266.84,203.54,46.11,4.91;16,355.45,195.26,94.50,7.16;16,357.02,210.65,35.53,6.39;16,356.62,224.10,72.66,6.39;16,356.62,233.66,63.61,6.39;16,356.62,243.21,78.66,6.39;16,356.62,252.77,64.07,6.39;16,356.62,262.33,4.95,6.39;16,445.05,212.08,11.42,4.91;16,356.62,203.45,46.19,4.91;16,459.91,210.69,35.52,6.39;16,459.52,222.79,77.18,6.39;16,459.52,230.70,48.64,6.39;16,459.52,238.60,49.90,6.39;16,459.52,246.51,71.30,6.39;16,459.52,254.42,72.45,6.39;16,459.52,262.33,4.95,6.39;16,459.52,203.50,46.11,4.91;16,159.60,294.35,85.11,7.07;16,161.15,309.52,35.06,6.30;16,160.76,321.46,83.01,6.30;16,160.76,329.27,73.20,6.30;16,160.76,337.07,79.29,6.30;16,160.76,344.88,83.22,6.30;16,160.76,352.68,50.21,6.30;16,160.76,360.49,4.88,6.30;16,50.89,395.07,80.66,6.30;16,50.89,402.88,73.41,6.30;16,50.89,410.68,19.28,6.30;16,248.02,310.94,11.27,4.85;16,160.76,302.43,45.58,4.85;16,212.17,410.95,92.83,7.07;16,212.17,426.17,36.65,6.30"><head>- 33 -</head><label>33</label><figDesc>We don't say that because: the small [hen]. art of the [hen]. a origami [hen]. a [hen] in a video game. a bad photo of the [hen]. ... -With CLIP templates: CLIP's top prediction: coucal -Score: 22.94 ....is a crow-like bird with a long tail and a loud call. ...is a bird with a long tail and a dark brown plumage... the small [coucal]. art of the [coucal]. a origami [coucal]. a [coucal] in a video game. a bad photo of the [coucal]. ... -With CLIP templates: Our top prediction: ostrich -Score: 29.14 ...can be identified by their long necks, long legs, and wings. ...by their long necks and legs, their large egg-laying body, and their lack of wings. ... ...are small, domesticated birds that are typically considered female.24.With GPT-3 prompts:CLIP's top prediction: coucalOverall score:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="16,302.40,426.07,11.27,4.85;16,300.61,445.40,11.27,4.85;16,300.31,464.72,11.27,4.85;16,214.93,419.19,54.76,4.85;16,147.90,426.77,37.57,17.95;16,348.97,440.62,37.57,17.95;16,261.93,309.57,35.06,6.30;16,261.54,321.51,76.91,6.30;16,261.54,329.31,48.74,6.30;16,261.54,337.11,50.00,6.30;16,261.54,344.92,71.11,6.30;16,261.54,352.72,72.24,6.30;16,261.54,360.53,4.88,6.30;16,261.54,302.47,45.50,4.85;16,348.99,294.31,97.75,7.07;16,350.53,309.48,35.06,6.30;16,357.07,322.84,78.17,6.30;16,357.07,332.27,72.62,6.30;16,357.07,341.71,80.34,6.30;16,357.07,351.14,65.19,6.30;16,357.07,360.57,4.88,6.30;16,437.41,310.90,11.27,4.85;16,350.14,302.39,45.58,4.85;16,452.07,309.52,35.06,6.30;16,453.26,321.46,78.37,6.30;16,453.26,329.27,50.21,6.30;16,453.26,337.07,51.46,6.30;16,453.26,344.88,72.57,6.30;16,453.26,352.68,73.70,6.30;16,453.26,360.49,4.88,6.30;16,451.68,302.43,45.50,4.85;16,161.15,386.41,89.65,7.17;16,162.72,401.80,35.55,6.39;16,162.32,415.26,69.97,6.39;16,162.32,424.83,58.28,6.39;16,162.32,434.40,75.59,6.39;16,162.32,443.96,60.65,6.39;16,162.32,453.53,4.95,6.39;16,50.90,488.56,90.41,6.39;16,50.90,496.47,84.14,6.39;16,252.51,403.24,11.43,4.92;16,162.32,394.60,46.23,4.92;16,214.47,504.66,94.15,7.17;16,214.47,520.09,37.17,6.39"><head>- 33 - 33 -</head><label>3333</label><figDesc>Score: 27.73 a photo of the small [ostrich]. art of the [ostrich]. a origami [ostrich]. a [ostrich] in a video game. a bad photo of the [ostrich]. ... -With CLIP templates: CLIP's top prediction: bustard -Score: 28.97 ...are a type of game bird with a heavy body and long legs. Large, long-necked bird with a big body and small head. ...24.With GPT-3 prompts: -Score: 27.89 a photo of the small [bustard]. art of the [bustard]. a origami [bustard]. a [bustard] in a video game. a bad photo of the [bustard]. ... -With CLIP templates:Our top prediction: goldfish -Score: 24.92 Goldfish are small, orange fish with shiny scales. The easiest way to identify a goldfish is by its color. ... ...are small, domesticted birds that are typically considered female.24.With GPT-3 prompts:CLIP's top prediction: coucalOverall score:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="16,305.97,520.00,11.43,4.92;16,304.15,539.60,11.43,4.92;16,303.85,559.20,11.43,4.92;16,217.26,513.02,55.54,4.92;16,149.29,520.70,38.10,18.21;16,353.20,534.75,38.10,18.21;16,253.08,401.84,35.55,6.39;16,252.77,413.95,81.68,6.39;16,252.77,421.87,53.12,6.39;16,252.77,429.78,54.39,6.39;16,252.77,437.70,75.80,6.39;16,252.77,445.61,76.94,6.39;16,252.77,453.53,4.95,6.39;16,252.68,394.65,46.14,4.92;16,353.22,386.37,104.68,7.17;16,354.78,401.76,35.55,6.39;16,354.39,413.87,62.59,6.39;16,354.39,421.78,78.34,6.39;16,354.39,429.70,78.73,6.39;16,354.39,437.61,77.83,6.39;16,354.39,445.53,34.07,6.39;16,354.39,453.44,4.95,6.39;16,442.89,403.20,11.43,4.92;16,354.39,394.56,46.23,4.92;16,451.66,401.80,35.55,6.39;16,451.27,413.91,69.38,6.39;16,451.27,421.82,14.09,6.39;16,451.27,429.74,56.54,6.39;16,451.27,437.65,64.64,6.39;16,451.27,445.57,14.09,6.39;16,451.27,453.48,4.95,6.39;16,451.27,394.60,46.14,4.92;16,150.30,486.52,103.87,7.40;16,151.92,502.41,36.70,6.60;16,151.51,514.91,80.95,6.60;16,151.51,523.09,42.56,6.60;16,151.51,531.26,89.45,6.60;16,151.51,539.43,86.25,6.60;16,151.51,547.60,19.09,6.60;16,151.51,555.77,5.11,6.60;16,36.48,591.98,84.45,6.60;16,36.48,600.15,76.86,6.60;16,36.48,608.32,20.19,6.60;16,242.87,503.90,11.80,5.08;16,151.51,494.98,47.72,5.08;16,205.34,608.60,97.19,7.40;16,205.34,624.53,38.37,6.60"><head></head><label></label><figDesc>the small [goldfish]. art of the [goldfish]. a origami [goldfish]. a [goldfish] in a video game. a bad photo of the [goldfish]. ... -With CLIP templates: CLIP's top prediction: coral reef -Score: 23.36 ...is a type of biotic reef developing in tropical waters. ...a large underwater structure made up of many small stony coral polyps. the small [coral reef]. art of the [coral reef]. a bad photo of the [coral reef]. ... -With CLIP templates: Our top prediction: house finch -Score: 26.84 House finches have red heads and red breasts. ...a small, plump songbird with a short tail and a wingspan of 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="16,299.81,624.44,11.80,5.08;16,297.93,644.67,11.80,5.08;16,297.62,664.90,11.80,5.08;16,208.22,617.23,57.34,5.08;16,138.05,625.16,39.33,18.80;16,348.57,639.67,39.33,18.80;16,257.44,502.46,36.70,6.60;16,257.03,514.96,70.53,6.60;16,257.03,523.13,17.96,6.60;16,257.03,531.30,64.10,6.60;16,257.03,539.47,69.04,6.60;16,257.03,547.64,17.96,6.60;16,257.03,555.81,5.11,6.60;16,257.03,495.03,47.64,5.08;16,348.58,486.48,97.63,7.40;16,350.20,502.37,36.70,6.60"><head></head><label></label><figDesc>the small [ouse finch]. art of the [house finch]. a bad photo of the [house finch]. ... -With CLIP templates: CLIP's top prediction: coucal -Score: 25.17</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17" coords="16,349.79,514.87,79.92,6.60;16,349.79,523.04,60.83,6.60;16,349.79,531.21,67.67,6.60;16,349.79,539.38,74.06,6.60;16,349.79,547.55,28.88,6.60;16,349.79,555.72,5.11,6.60;16,441.15,503.85,11.80,5.08;16,349.79,494.94,47.72,5.08;16,456.51,502.41,57.68,6.60;16,456.10,514.91,79.74,6.60;16,456.10,523.09,50.25,6.60;16,456.10,531.26,51.56,6.60;16,456.10,539.43,73.67,6.60;16,456.10,547.60,74.85,6.60;16,456.10,555.77,5.11,6.60;16,456.10,494.98,47.64,5.08"><head>33 -</head><label>33</label><figDesc>...a black bird with a long tail that is native to Africa ...a species of bird that is typically dark in color with a long tail. ... 24.With GPT-3 prompts: -Overall score: 25.48 a photo of the small [coucal]. art of the [coucal]. a origami [coucal]. a [coucal] in a video game. a bad photo of the [coucal]. ... -With CLIP templates:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18" coords="16,85.55,585.60,424.13,8.12;16,59.23,477.13,77.18,88.51"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Additional Visualization of GPT-3's Prompts for CLIP. Above examples are from the ImageNet dataset.</figDesc><graphic coords="16,59.23,477.13,77.18,88.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,308.86,644.74,236.25,68.41"><head></head><label></label><figDesc>Besides the two keys, we convert the few-shot training labels into one-hot encodings L</figDesc><table /><note>onehot ‚àà R N (K+K )√óN , and regard them as the same values for both keys. During training, we follow Tip-Adapter that only enables the cached keys in the adapter to be learnable and keeps the pre-trained models frozen. DINO Visual Encoder ùëê ùëé ùë° ùëë ùëú ùëî ùëù ùëé ùëõ ùëë ùëé ùëü ùëé ùëè ùëè ùëñùë° ùëáùëíùë†ùë° ùêºùëöùëéùëîùëí ùë≠ ,-./ ùë≠ 0.12</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,50.11,67.55,236.25,377.16"><head>Table 1 .</head><label>1</label><figDesc>Efficiency Comparison on ImageNet. We test the training time with a single A100 GPU under 16-shot setting.</figDesc><table coords="6,50.11,67.55,236.25,340.27"><row><cell></cell><cell>70</cell><cell></cell><cell cols="2">ImageNet</cell></row><row><cell></cell><cell>68</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>66</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Score(%)</cell><cell>62 64</cell><cell></cell><cell></cell><cell cols="2">Zero-shot CLIP</cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell cols="2">CoOp CLIP-Adapter</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Tip-Adapter-F</cell></row><row><cell></cell><cell>58</cell><cell></cell><cell></cell><cell cols="2">CALIP-FS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CaFo</cell></row><row><cell></cell><cell>56</cell><cell cols="4">0 1 2 Number of training samples per class 4 8 16</cell></row><row><cell cols="6">Figure 5. Performance (%) Comparison on ImageNet. We</cell></row><row><cell cols="6">compare CaFo with other methods for different few-shot settings.</cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell>Epochs</cell><cell>Time</cell><cell>Accuracy Gain</cell></row><row><cell cols="3">Zero-shot CLIP</cell><cell>0</cell><cell>0</cell><cell>60.33</cell><cell>-</cell></row><row><cell cols="3">Zero-shot CALIP</cell><cell>0</cell><cell>0</cell><cell>60.57</cell><cell>-</cell></row><row><cell cols="3">Linear-probe CLIP</cell><cell>-</cell><cell>13min</cell><cell>56.13</cell><cell>-4.20</cell></row><row><cell>CoOp</cell><cell></cell><cell></cell><cell cols="3">200 14h 40min 62.95 +2.62</cell></row><row><cell cols="3">CLIP-Adapter</cell><cell>200</cell><cell>50min</cell><cell>63.59 +3.26</cell></row><row><cell cols="3">Tip-Adapter-F</cell><cell>20</cell><cell>5min</cell><cell>65.51 +5.18</cell></row><row><cell cols="2">CALIP-FS</cell><cell></cell><cell>200</cell><cell>1h</cell><cell>65.81 +5.48</cell></row><row><cell>CaFo</cell><cell></cell><cell></cell><cell>20</cell><cell>10min</cell><cell>68.79 +8.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,50.11,88.81,495.03,624.35"><head>Table 2 ,</head><label>2</label><figDesc>CaFo surpasses all existing methods for different shot settings. Remarkably, CaFo with 1 shot even outperforms the 8-shot Linear-probe CLIP and CoOp, and CaFo with 8 shots is better than all methods with 16 shots. For zero-shot learning, CaFo sigiificantly surpasses CLIP and CALIP, demonstrating the importance of DALL-E's generation. In Table1, we present the efficiency of CaFo concerning training epochs and time. Our CaFo achieves the best performance-efficiency tradeoff with 68.79% accuracy and only 10 minutes training.</figDesc><table coords="6,310.85,88.81,234.29,130.57"><row><cell>Shot</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell></row><row><cell>Zero-shot CLIP</cell><cell cols="2">60.33 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Zero-shot CALAP 60.57 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">Linear-probe CLIP -22.17 31.90 41.20 49.52 56.13</cell></row><row><cell>CoOp</cell><cell cols="6">-57.15 57.81 59.99 61.56 62.95</cell></row><row><cell>CLIP-Adapter</cell><cell cols="6">-61.20 61.52 61.84 62.68 63.59</cell></row><row><cell>VT-CLIP</cell><cell cols="6">-60.53 61.29 62.02 62.81 63.92</cell></row><row><cell>Tip-Adapter-F</cell><cell cols="6">-61.32 61.69 62.52 64.00 65.51</cell></row><row><cell>CALIP-FS</cell><cell cols="6">-61.35 62.03 63.13 64.11 65.81</cell></row><row><cell>CaFo</cell><cell cols="6">62.99 63.80 64.34 65.64 66.86 68.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,308.86,234.84,236.25,176.10"><head>Table 2 .</head><label>2</label><figDesc>Quantative Performance (%) Comparison on Ima-geNet. For zero-shot performance, CaFo is trained with images generated by DALL-E without any few-shot data.</figDesc><table coords="6,316.83,286.93,209.86,124.01"><row><cell>Datasets</cell><cell>Source</cell><cell cols="2">Target</cell></row><row><cell></cell><cell>ImageNet</cell><cell>-V2</cell><cell>-Sketch</cell></row><row><cell>Zero-shot CLIP</cell><cell>60.33</cell><cell>53.27</cell><cell>35.44</cell></row><row><cell>Zero-shot CALIP</cell><cell>60.57</cell><cell>53.70</cell><cell>35.61</cell></row><row><cell>CoOp</cell><cell>62.95</cell><cell>54.58</cell><cell>31.04</cell></row><row><cell>CLIP-Adapter</cell><cell>63.59</cell><cell>55.69</cell><cell>35.68</cell></row><row><cell>CALIP-FS</cell><cell>65.81</cell><cell>55.98</cell><cell>35.37</cell></row><row><cell>Tip-Adapter-F</cell><cell>65.51</cell><cell>57.11</cell><cell>36.00</cell></row><row><cell>CaFo</cell><cell>68.79</cell><cell>57.99</cell><cell>39.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,308.86,426.41,236.25,166.57"><head>Table 3 .</head><label>3</label><figDesc>Distribution</figDesc><table coords="6,308.86,476.35,236.25,116.62"><row><cell>On Other Datasets. To further assess the robustness in</cell></row><row><cell>different scenarios, we test CaFo on extra 10 datasets in Fig-</cell></row><row><cell>ure 6. For different semantic domains including real-world</cell></row><row><cell>scenes, detailed textures, and satellite-captured landscapes,</cell></row><row><cell>CaFo consistently shows leading performance and indicates</cell></row><row><cell>excellent robustness via the collaboration of diverse knowl-</cell></row><row><cell>edge. Notably, on some datasets, e.g., Caltech101 and Ox-</cell></row><row><cell>fordPets, the zero-shot CaFo perform even comparably to</cell></row><row><cell>other methods with 4 shots, demonstrating the effectiveness</cell></row><row><cell>of zero-shot DALL-E for few-shot data expansion.</cell></row></table><note>Shift (%) Comparison. We train the models on "Source" dataset and test on "Target" datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,50.11,174.44,495.00,538.49"><head>Table 4 .</head><label>4</label><figDesc>Ablation Study (%) of Cascaded Models. We ablate different pre-trained models on ImageNet with 1, 4, and 16 shots.</figDesc><table coords="7,279.14,174.44,249.46,89.72"><row><cell>FGVCAircraft</cell><cell>92</cell><cell>OxfordPets</cell><cell>80</cell><cell>Food101</cell></row><row><cell></cell><cell>90</cell><cell></cell><cell>78</cell><cell></cell></row><row><cell>Score(%)</cell><cell>86 88</cell><cell>Score(%)</cell><cell>74 76</cell><cell></cell></row><row><cell>Zero-shot CLIP</cell><cell></cell><cell>Zero-shot CLIP</cell><cell></cell><cell>Zero-shot CLIP</cell></row><row><cell>CoOp</cell><cell></cell><cell>CoOp</cell><cell></cell><cell>CoOp</cell></row><row><cell>CLIP-Adapter</cell><cell>84</cell><cell>CLIP-Adapter</cell><cell>72</cell><cell>CLIP-Adapter</cell></row><row><cell>Tip-Adapter-F</cell><cell></cell><cell>Tip-Adapter-F</cell><cell></cell><cell>Tip-Adapter-F</cell></row><row><cell>CaFo</cell><cell></cell><cell>CaFo</cell><cell></cell><cell>CaFo</cell></row><row><cell></cell><cell>82</cell><cell>0 1 2 Number of training samples per class 4 8 16</cell><cell>70</cell><cell>0 1 2 Number of training samples per class 4 8 16</cell></row></table><note>Figure 6. Performance (%) Comparison on 10 Datasets. Our method shows state-of-the-art performance for all few-shot settings on different datasets, which indicates superior generalization capacity.Generated Number via DALL-E. We utilize DALL-E to generate synthetic images as the expanded few-shot training data. In Table6, we explore the best synthetic number K for each cat-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,308.86,456.01,236.25,19.08"><head>Table 5 .</head><label>5</label><figDesc>Ablation Study (%) of Adaptive Inference. We conduct different ensemble methods of cache model on ImageNet.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,50.11,73.51,236.25,545.72"><head>Table 6 .</head><label>6</label><figDesc>Ablation Study (%) of Generated Number via DALL-E. We compare different shot numbers on ImageNet. line performs the best, since pZS itself shows strong transfer ability and can effectively suppress the wrong predictions of other logits.</figDesc><table coords="8,50.11,73.51,236.25,545.72"><row><cell></cell><cell>Tench</cell><cell cols="2">Great White</cell><cell cols="2">Black Swan Toilet Paper</cell><cell>Bolete</cell></row><row><cell></cell><cell></cell><cell cols="2">Shark</cell><cell></cell><cell></cell></row><row><cell>ImageNet</cell><cell>data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DALL-E</cell><cell>data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Abyssinian</cell><cell>Beagle</cell><cell cols="2">Bengal</cell><cell>Boxer</cell></row><row><cell>OxfordPets</cell><cell>data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DALL-E</cell><cell>data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Accordion</cell><cell></cell><cell>Anchor</cell><cell></cell><cell>Ant</cell><cell>Binocular</cell></row><row><cell>Caltech101</cell><cell>data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DALL-E</cell><cell>data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DALL-E</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell></row><row><cell>1</cell><cell cols="2">63.29</cell><cell>64.06</cell><cell>65.11</cell><cell>66.48</cell><cell>68.64</cell></row><row><cell>2</cell><cell cols="2">63.66</cell><cell>64.34</cell><cell>65.37</cell><cell>66.86</cell><cell>68.79</cell></row><row><cell>4</cell><cell cols="2">63.71</cell><cell>64.33</cell><cell>65.35</cell><cell>66.75</cell><cell>68.61</cell></row><row><cell>8</cell><cell cols="2">63.80</cell><cell>64.26</cell><cell>65.64</cell><cell>66.68</cell><cell>68.76</cell></row><row><cell>16</cell><cell cols="2">63.68</cell><cell>64.16</cell><cell>65.40</cell><cell>66.57</cell><cell>68.41</cell></row><row><cell cols="7">CLIP's Visual Encoders. We conduct CaFo with different</cell></row><row><cell cols="7">CLIP's visual encoders for comparison with other methods. As</cell></row><row><cell cols="7">shown in Table 7, CaFo consistently achieves leading performance</cell></row><row><cell cols="7">with different visual backbones, indicating our generalizability to</cell></row><row><cell cols="3">network architectures.</cell><cell></cell><cell></cell><cell></cell></row></table><note>Figure 7. Visualizations of DALL-E's Generated Images. Examples are from ImageNet, OxfordPets and Caltech101 datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,308.86,417.53,236.25,86.00"><head>Table 7 .</head><label>7</label><figDesc>Ablation Study (%) of CLIP's Visual Encoders. We experiment different visual backbones on the 16-shot ImageNet.</figDesc><table coords="8,321.63,417.53,204.06,51.55"><row><cell cols="2">Zero-shot CLIP 60.33 62.53</cell><cell>63.80</cell><cell>68.73</cell></row><row><cell>CoOp</cell><cell>62.95 66.60</cell><cell>66.85</cell><cell>71.92</cell></row><row><cell>CLIP-Adapter</cell><cell>63.59 65.39</cell><cell>66.19</cell><cell>71.13</cell></row><row><cell>Tip-Adapter-F</cell><cell>65.51 68.56</cell><cell>68.65</cell><cell>73.69</cell></row><row><cell>CaFo</cell><cell>68.79 70.86</cell><cell>70.82</cell><cell>74.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="14,50.11,174.44,495.00,466.30"><head>Table 10 .</head><label>10</label><figDesc>Performance (%) Comparison on 10 Datasets. Our method shows state-of-the-art performance for all few-shot settings on different datasets. 'CaFo w/o D.&amp;G.' denotes CaFo without DALL-E's generated images and GPT3's created prompts. Ablation Study (%) of Zero-shot CaFo via DALL-E on Different Datasets. We leverage DALL-E to generate different numbers of synthetic images for zero-shot recognition.</figDesc><table coords="14,279.14,174.44,249.46,90.74"><row><cell>FGVCAircraft</cell><cell></cell><cell>92</cell><cell>OxfordPets</cell><cell></cell><cell>80</cell><cell>Food101</cell></row><row><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell>78</cell></row><row><cell>Zero-shot CLIP CoOp</cell><cell>Score(%)</cell><cell>86 88</cell><cell>Zero-shot CLIP CoOp</cell><cell>Score(%)</cell><cell>74 76</cell></row><row><cell>CLIP-Adapter</cell><cell></cell><cell></cell><cell>CLIP-Adapter</cell><cell></cell><cell></cell><cell>Zero-shot CLIP</cell></row><row><cell>Tip-Adapter-F</cell><cell></cell><cell></cell><cell>Tip-Adapter-F</cell><cell></cell><cell></cell><cell>CoOp</cell></row><row><cell>CaFo w/o D.&amp;G.</cell><cell></cell><cell>84</cell><cell>CaFo w/o D.&amp;G.</cell><cell></cell><cell>72</cell><cell>CLIP-Adapter</cell></row><row><cell>CaFo w/o G.</cell><cell></cell><cell></cell><cell>CaFo w/o G.</cell><cell></cell><cell></cell><cell>Tip-Adapter-F</cell></row><row><cell>CaFo</cell><cell></cell><cell></cell><cell>CaFo</cell><cell></cell><cell></cell><cell>CaFo</cell></row><row><cell></cell><cell></cell><cell>82</cell><cell>0 1 2 Number of training samples per class 4 8 16</cell><cell></cell><cell>70</cell><cell>0 1 2 Number of training samples per class 4 8 16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="15,53.93,202.98,227.47,122.73"><head>Our top prediction: great white shark</head><label></label><figDesc></figDesc><table coords="15,53.93,211.07,208.56,114.64"><row><cell>-With GPT-3 prompts:</cell><cell></cell></row><row><cell>-Score: 26.39</cell><cell>24.33</cell></row><row><cell>...is large, with a dark gray upper</cell><cell></cell></row><row><cell>body and white underside.</cell><cell></cell></row><row><cell>...can be identified by its large</cell><cell></cell></row><row><cell>size, wide-set eyes, and</cell><cell></cell></row><row><cell>distinctive white belly.</cell><cell></cell></row><row><cell>...</cell><cell></cell></row><row><cell>...are small, domesticated birds</cell><cell></cell></row><row><cell>that are typically considered</cell><cell></cell></row><row><cell>female.</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Performance Comparison</head><p>In Figure <ref type="figure" coords="13,100.62,94.85,3.36,7.77">9</ref>, we compare the performance of CaFo without DALL-E <ref type="bibr" coords="13,84.66,105.81,13.95,7.77" target="#b57">[57]</ref>'s generated images or GPT-3 <ref type="bibr" coords="13,209.57,105.81,9.86,7.77" target="#b3">[4]</ref>'s created prompts on 10 datasets, which still consistently outperform the second-best Tip-Adapter-F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Ablation Study</head><p>Other Foundation Models. For the cache model, we investigate other pre-trained foundation models besides CLIP <ref type="bibr" coords="13,255.96,183.70,14.94,7.77" target="#b56">[56]</ref> and DINO <ref type="bibr" coords="13,74.52,194.66,9.52,7.77" target="#b6">[7]</ref>, including SimCLR <ref type="bibr" coords="13,159.02,194.66,13.74,7.77" target="#b10">[11]</ref>, MAE <ref type="bibr" coords="13,200.16,194.66,13.74,7.77" target="#b29">[29]</ref>, and SLIP <ref type="bibr" coords="13,254.76,194.66,13.74,7.77" target="#b50">[50]</ref>. We preserve the prompting and generation by GPT-3 <ref type="bibr" coords="13,231.00,205.61,10.45,7.77" target="#b3">[4]</ref> and DALL-E <ref type="bibr" coords="13,59.01,216.57,13.74,7.77" target="#b57">[57]</ref>, along with We the pZS as the ensemble baseline during adaptive inference. As shown in We report the accuracy of 1 and 16 shots on ImageNet <ref type="bibr" coords="13,87.71,392.10,13.74,7.77" target="#b14">[15]</ref>, OxfordPets <ref type="bibr" coords="13,150.21,392.10,13.74,7.77" target="#b53">[53]</ref>, and EuroSAT <ref type="bibr" coords="13,220.44,392.10,13.74,7.77" target="#b34">[34]</ref>.</p><p>Zero-shot CaFo. As we leverage the pre-trained DALL-E to generate the supplementary few-shot training set in a zero-shot manner, our CaFo can be evaluated under zero-shot settings the same as CLIP, for which none of the human-annotated training images is given. In Table <ref type="table" coords="13,150.10,469.59,7.47,7.77">10</ref>, we report the best generated image number K of DALL-E for zero-shot CaFo. The number "0" denotes Zero-shot CLIP. For different datasets, the best number varies ranging from 1‚àº16, and the larger number normally cannot get the better result, probably due to the low-quality synthetic images. On Caltech101 <ref type="bibr" coords="13,140.35,524.39,14.94,7.77" target="#b18">[19]</ref> and EuroSAT <ref type="bibr" coords="13,210.45,524.39,13.74,7.77" target="#b34">[34]</ref>, zero-shot CaFo largely surpasses CLIP by +4.62% and +7.54%, indicating our superiority under zero-shot settings.</p><p>Hyperparameter Œ≤. In Formula 5 and 6, we utilize a non-</p><p>for the affinity matrix of CLIP and DINO in the cache model, where Œ≤ controls the matrix sharpness. In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Visualization</head><p>GPT-3's Prompts for CLIP. In Figure <ref type="figure" coords="13,471.87,94.06,8.97,7.77">12</ref> and 13, we show more visualization of the prompts produced by GPT-3 and how they assist our CaFo to rectify false predictions of the original CLIP's templates.</p><p>DALL-E's Generated Images. In Figure <ref type="figure" coords="13,484.64,151.01,7.47,7.77">14</ref>, we visualize more synthetic images generated by DALL-E on different datasets. Benefited from the pre-trained DALL-E, the generated images can well highlight the semantics of target category and effectively expand the few-shot training set in low-data regimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t-SNE.</head><p>We present the t-SNE visualization of our CaFo and the second-best Tip-Adapter-F in Figure <ref type="figure" coords="13,444.83,229.87,7.47,7.77">10</ref>. CaFo shows more contrastive distribution of category clusters and well mitigates some aliasing between similar classes.</p><p>Learning Curves. In Figure <ref type="figure" coords="13,432.10,275.86,7.47,7.77">11</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,70.04,164.92,216.32,7.77;9,70.03,175.88,216.33,7.77;9,70.03,186.84,216.33,7.77;9,70.03,197.64,216.33,7.93;9,70.03,208.60,216.33,7.73;9,70.03,219.56,143.10,7.93" xml:id="b0">
	<analytic>
		<title level="a" type="main">CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding</title>
		<author>
			<persName coords=""><forename type="first">Mohamed</forename><surname>Afham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isuru</forename><surname>Dissanayake</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dinithi</forename><surname>Dissanayake</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amaya</forename><surname>Dharmasiri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kanchana</forename><surname>Thilakarathna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ranga</forename><surname>Rodrigo</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.00967</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">2022</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,232.45,216.32,7.77;9,70.03,243.41,216.33,7.77;9,70.03,254.21,216.33,7.93;9,70.03,265.17,216.33,7.73;9,70.03,276.13,97.85,7.93" xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.279</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">December 2015</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,289.02,216.32,7.77;9,70.03,299.98,216.33,7.77;9,70.03,310.78,216.33,7.93;9,70.03,321.90,98.76,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main">Food-101 ‚Äì Mining Discriminative Components with Random Forests</title>
		<author>
			<persName coords=""><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10599-4_29</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ‚Äì ECCV 2014</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,334.64,216.32,7.77;9,70.03,345.60,216.33,7.77;9,70.03,356.55,216.33,7.77;9,70.03,367.35,216.33,7.93;9,70.03,378.31,216.33,7.93;9,70.03,389.43,17.93,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,402.17,216.32,7.77;9,70.03,413.13,216.33,7.77;9,70.03,423.92,216.33,7.93;9,70.03,434.88,194.33,7.93" xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Clustering for Unsupervised Learning of Visual Features</title>
		<author>
			<persName coords=""><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01264-9_9</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ‚Äì ECCV 2018</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018-09">September 2018</date>
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,447.78,216.32,7.77;9,70.03,458.74,216.33,7.77;9,70.03,469.54,216.33,7.93;9,70.03,480.49,216.33,7.93;9,70.03,491.61,50.30,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised Pre-Training of Image Features on Non-Curated Data</title>
		<author>
			<persName coords=""><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00305</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,504.35,216.33,7.77;9,70.03,515.31,216.33,7.77;9,70.03,526.27,216.33,7.77;9,70.03,537.07,216.33,7.73;9,70.03,548.02,216.33,7.93;9,70.03,559.14,44.83,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main">Emerging Properties in Self-Supervised Vision Transformers</title>
		<author>
			<persName coords=""><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.00951</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10">October 2021</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,571.88,216.32,7.77;9,70.03,582.84,216.33,7.77;9,70.03,593.80,216.33,7.77;9,70.03,604.59,216.33,7.93;9,70.03,615.55,216.33,7.93;9,70.03,626.67,27.89,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName coords=""><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
			<idno type="ORCID">0000-0001-8564-374X</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2017.2699184</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">2160-9292</idno>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018-04-01">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,639.41,216.32,7.77;9,70.03,650.37,216.33,7.77;9,70.03,661.33,216.33,7.77;9,70.03,672.12,216.33,7.93;9,70.03,683.08,216.33,7.93;9,70.03,694.04,216.33,7.93;9,70.03,705.16,74.21,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName coords=""><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Hal</forename><surname>Daum√©</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="9,328.79,76.13,216.32,7.77;9,328.78,87.08,216.33,7.77;9,328.78,98.04,27.89,7.77" xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName coords=""><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,109.37,216.32,7.77;9,328.78,120.33,216.33,7.77;9,328.78,131.12,155.82,7.93" xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName coords=""><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<idno>2020. 13</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,328.79,142.61,216.32,7.77;9,328.78,153.57,216.33,7.77;9,328.78,164.37,216.33,7.93;9,328.78,175.32,216.33,7.73;9,328.78,186.28,157.38,7.93" xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning</title>
		<author>
			<persName coords=""><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.00893</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10">October 2021</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,197.77,216.32,7.77;9,328.78,208.73,216.33,7.77;9,328.78,219.52,216.34,7.93;9,328.78,230.48,208.28,7.93" xml:id="b12">
	<analytic>
		<title level="a" type="main">Describing Textures in the Wild</title>
		<author>
			<persName coords=""><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2014.461</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-06">2014</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,241.97,216.32,7.77;9,328.78,252.93,216.33,7.77;9,328.78,263.89,118.28,7.77" xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Dayma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suraj</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pedro</forename><surname>Cuenca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Khalid</forename><surname>Saifullah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tanishq</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Phuc</forename><surname>Le Khac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Melas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ritobrata</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dall‚Ä¢e mini</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,275.21,216.32,7.77;9,328.78,286.17,216.33,7.77;9,328.78,296.97,216.33,7.93;9,328.78,307.93,216.33,7.93;9,328.78,319.05,8.97,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Kai Li</surname></persName>
		</author>
		<author>
			<persName><surname>Li Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009-06">2009. 2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,330.37,216.32,7.77;9,328.78,341.33,216.33,7.77;9,328.78,352.29,216.33,7.77;9,328.78,363.25,216.33,7.77;9,328.78,374.21,216.33,7.77;9,328.78,385.00,216.33,7.93;9,328.78,395.96,216.33,7.93;9,328.78,407.08,70.96,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main">Cogview: Mastering textto-image generation via transformers</title>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,418.41,216.32,7.77;9,328.78,429.36,216.33,7.77;9,328.78,440.16,216.33,7.93;9,328.78,451.12,171.66,7.93" xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised Visual Representation Learning by Context Prediction</title>
		<author>
			<persName coords=""><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.167</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">December 2015</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,462.61,216.32,7.77;9,328.78,473.57,216.33,7.77;9,328.78,484.52,216.33,7.77;9,328.78,495.48,216.33,7.77;9,328.78,506.44,216.33,7.77;9,328.78,517.40,51.28,7.77" xml:id="b17">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,528.72,216.32,7.77;9,328.78,539.68,216.33,7.77;9,328.78,550.64,216.33,7.77;9,328.78,561.44,216.33,7.73;9,328.78,572.40,172.08,7.93" xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories</title>
		<author>
			<persName><surname>Li Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2004.383</idno>
	</analytic>
	<monogr>
		<title level="m">2004 Conference on Computer Vision and Pattern Recognition Workshop</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,583.88,216.32,7.77;9,328.78,594.84,216.33,7.77;9,328.78,605.64,216.33,7.93;9,328.78,616.76,101.62,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName coords=""><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,628.09,216.32,7.77;9,328.78,639.04,216.33,7.77;9,328.78,649.84,216.33,7.93;9,328.78,660.80,216.33,7.73;9,328.78,671.76,216.33,7.93;9,328.78,682.88,166.61,7.77" xml:id="b20">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName coords=""><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017-08">Aug 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="9,328.79,694.20,216.32,7.77;9,328.78,705.16,216.33,7.77" xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rongyao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.03,76.13,216.33,7.77;10,70.03,87.08,81.16,7.77" xml:id="b22">
	<analytic>
		<title level="a" type="main">K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</title>
		<author>
			<persName><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.121</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,98.38,216.32,7.77;10,70.03,109.18,216.33,7.93;10,70.03,120.14,129.17,7.93" xml:id="b23">
	<analytic>
		<title level="a" type="main">Making Pre-trained Language Models Better Few-shot Learners</title>
		<author>
			<persName coords=""><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
		<idno type="arXiv">arXiv:2012.15723</idno>
		<idno>2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.04,131.59,216.33,7.77;10,70.03,142.55,216.33,7.77;10,70.03,153.51,216.33,7.77;10,70.03,164.47,216.33,7.77;10,70.03,175.27,216.34,7.93;10,70.03,186.23,167.70,7.93" xml:id="b24">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName coords=""><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florent</forename><surname>Altch√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,197.68,216.33,7.77;10,70.03,208.64,216.33,7.77;10,70.03,219.60,216.33,7.77;10,70.03,230.56,216.33,7.77;10,70.03,241.52,216.33,7.77;10,70.03,252.48,216.33,7.77;10,70.03,263.28,216.33,7.93;10,70.03,274.24,216.33,7.93;10,70.03,285.36,172.07,7.77" xml:id="b25">
	<analytic>
		<title level="a" type="main">(2021) Volume 2, Issue 4 Cultural Implications of China Pakistan Economic Corridor (CPEC Authors: Dr. Unsa Jamshed Amar Jahangir Anbrin Khawaja Abstract: This study is an attempt to highlight the cultural implication of CPEC on Pak-China relations, how it will align two nations culturally, and what steps were taken by the governments of two states to bring the people closer. After the establishment of diplomatic relations between Pakistan and China, the cultural aspect of relations between the two states also moved forward. The flow of cultural delegations intensified after the 2010, because this year was celebrated as the ‚ÄòPak-China Friendship Year‚Äô. This dimension of relations further cemented between the two states with the signing of CPEC in April 2015. CPEC will not only bring economic prosperity in Pakistan but it will also bring two states culturally closer. The roads and other communication link under this project will become source of cultural flow between the two states. Keyswords: China, CPEC, Culture, Exhibitions Pages: 01-11 Article: 1 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)01 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)01 Download Pdf: download pdf view article Creative Commons License Political Persona on Twittersphere: Comparing the Stardom of Prime Minister(s) of Pakistan, UK and India Authors: Maryam Waqas Mudassar Hussain Shah Saima Kausar Abstract: Political setup demands to use Twittersphere for preserving its reputation because of significant twitter audience, which follows celebrities and political figures. In this perspective, political figures frequently use twitter to highlight their political as well as personal lives worldwide. However, political figures take the stardom status among the twitter audience that follow, retweet and comment by their fans. The purpose of this study is, to analyze what kind of language, level of interest is made by political figures while communicating via twitter, text, phrases and languages used by political figures, and do their tweets contribute in their reputation. The qualitative content analysis is used for evaluation of the interests shared by PM Imran Khan, PM Boris John Son and PM Narendra Modi with the key words of tweets. A well-established coding sheet is developed for the analysis of text, phrases and words in the frames of negative, positive and neutral from March 2020 to May 2020. The results are demonstrating on the basis of content shared by Prime Ministers of three countries i.e., From Pakistan, Imran Khan, United Kingdom, Johnson Boris and India, Narendra Modi on twitter. The findings also reveal that varied issues discussed in tweets, significantly positive and neutral words are selected by these political figures. PM Imran tweeted more negative tweets than PM Boris Johnson and PM Narendra Modi. However, PM Boris Johnson and PM Narendra Modi make significant positive and neutral tweets. It is observed that political figures are conscious about their personal reputation while tweeting. It also revealed that the issues and tweets shared by these leaders contribute to their personal reputation. Keyswords: Imran Khan, Johnson Boris, Narendra Modi, Political Persona, Stardom, Twittersphere Pages: 12-23 Article: 2 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)02 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)02 Download Pdf: download pdf view article Creative Commons License An Empirical Relationship between Government Size and Economic Growth of Pakistan in the Presence of Different Budget Uncertainty Measures Authors: Sunila Jabeen Dr. Wasim Shahid Malik Abstract: Relationship between government size and economic growth has always been a debated issue all over the world since the formative work of Barro (1990). However, this relationship becomes more questionable when policy uncertainty is added in it. Hence, this paper presents evidence on the effect of government size on economic growth in the presence of budget uncertainty measured through three different approaches. Rather than relying on the traditional and complicated measures of uncertainty, a new method of measuring uncertainty based on government budget revisions of total spending is introduced and compared with the other competing approaches. Using time series annual data from 1973-2018, the short run and long run coefficients from Autoregressive Distributed Lag (ARDL) framework validate the negative effect of budget uncertainty and government size on economic growth of Pakistan regardless of the uncertainty measure used. Therefore, to attain the long run economic growth, along with the control on the share of government spending in total GDP, government should keep the revisions in the budget as close to the initial announcements as it can so that uncertainty can be reduced. Further, the uncertainty in fiscal spending calculated through the deviation method raises a big question on the credibility of fiscal policy in Pakistan. Higher will be the deviation higher will be the uncertainty and lower the fiscal policy credibility hence making fiscal policy less effective in the long run. Keyswords: Budget Uncertainty, Economic Growth, Government Size, Policy Credibility Pages: 24-38 Article: 3 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)03 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)03 Download Pdf: download pdf view article Creative Commons License Despair in The Alchemist by Ben Jonson Authors: Dr. Fatima Syeda Dr. Faiza Zaheer Numrah Mehmood Abstract: This research aims to challenge the assumption that The Alchemist by Ben Jonson is one of the greatest examples of the ‚Äúexplicit mirth and laughter‚Äù (Veneables 86). The paper argues that The Alchemist is a cynical and despairing play created in an atmosphere not suitable for a comedy. This is a qualitative study of the text and aims at an analysis of the theme, situations, characters, language, and the mood of the play to determine that Jonson is unable to retain the comic spirit in The Alchemist and in an attempt to ‚Äúbetter men‚Äù (Prologue. 12) he becomes more satirical and less humorous or comic. This research is important for it contends that the play, termed as a comedy, may be read as a bitter satire on the cynical, stinky, and despairing world of the Elizabethan times. Keyswords: Comedy, Despair, Reformation Pages: 39-47 Article: 4 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)04 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)04 Download Pdf: download pdf view article Creative Commons License Analysis of Principles of Coordinated Border Management (CBM) in articulation of War-Control Strategies: An Account of Implementation Range on Pakistan and Afghanistan Authors: Dr. Sehrish Qayyum Dr. Umbreen Javaid Abstract: Currently, Border Management is crucial issue not only for Pakistan but for the entire world due to increased technological developments and security circumstances. Pakistan and Afghanistan being immediate states have inter-connected future with socio-economic and security prospects. Principles of Coordinated Border Management (CBM) approach have been extracted on the basis of in-depth interviews with security agencies and policymakers to understand the real time needs. The current research employs mixed method approach. Process Tracing is employed in this research to comprehend the causal mechanism behind the contemporary issue of border management system. A detailed statistical analysis of prospect outcomes has been given to validate the implication of CBM. Implication range of CBM has been discussed with positive and probably negative impacts due to its wide range of significance. This research gives an analysis of feasibility support to exercise CBM in best interest of the state and secure future of the region. Keyswords: Afghanistan, Coordinated Border Management, Fencing, Pakistan, Security Pages: 48-62 Article: 5 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)05 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)05 Download Pdf: download pdf view article Creative Commons License The Belt and Road Initiative (BRI) vs. Quadrilateral Security Dialogue (the Quad): A Perspective of a Game Theory Authors: Muhammad Atif Prof. Dr. Muqarrab Akbar Abstract: Containment is the central part of the U.S.&apos;s foreign policy during the cold war. With the application of containment Policy, the U.S. achieved much success in international politics. Over time China has become more powerful and sees great power in international politics. China wants to expand and launched the Belt and Road Initiative (BRI). The primary purpose of The Belt and Road Initiative (BRI) is to achieve support from regional countries and save their interests from the U.S. In 2017, the American administration launched its Containment policy through Quadrilateral Security Dialogue (the Quad) to keep their interest from China. The Quadrilateral Security Dialogue (Quad) is comprising of Australia, the United States, Japan, and India. This Study is based on Qualitative research with theoretical application of Game theory. This research investigates both plans of China (BRI) and the U.S. (the Quad) through a Game Theory. In this study, China and the U.S. both like to act as gamers in international politics. This study recommends that Game theory can predict all developments in the long term. Keyswords: Containment, Expansionism, Quadrilateral Security Dialogue, The Belt and Road Initiative (BRI) Pages: 63-75 Article: 6 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)06 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)06 Download Pdf: download pdf view article Creative Commons License Narendra Modi a Machiavellian Prince: An Appraisal Authors: Dr. Imran Khan Dr. Karim Haider Syed Muhammad Yousaf Abstract: The comparison of Narendra Modi and Machiavellian Prince is very important as policies of Modi are creating problems within India and beyond the borders. The Prince is the book of Niccolo Machiavelli a great philosopher of his time. If Indian Prime Minister Narendra Modi qualifies as a Prince of Machiavelli is a very important question. This is answered in the light of his policies and strategies to become the undisputed political leader of India. Much of the Machiavellian Prince deals with the problem of how a layman can raise himself from abject and obscure origins to such a position that Narendra Modi has been holding in India since 2014. The basic theme of this article is revolving around the question that is following: Can Modi‚Äôs success be attributed to techniques of The Prince in important respects? This article analyzed Narendra Modi&apos;s policies and strategies to develop an analogy between Machiavellian Prince and Modi in terms of characteristics and political strategies. This research work examines, how Narendra Modi became the strongest person in India. Keyswords: Comparison, India, Machiavelli, Modus Operandi, Narendra Modi Pages: 76-84 Article: 7 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)07 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)07 Download Pdf: download pdf view article Creative Commons License Analyzing Beckett&apos;s Waiting for Godot as a Political Comedy Authors: Muhammad Umer Azim Dr. Muhammad Saleem Nargis Saleem Abstract: This study was devised to analyze Samuel Beckett‚Äôs play Waiting for Godot in the light of Jean-Francois Lyotard‚Äôs theory of postmodernism given in his book The Postmodern Condition (1984). This Lyotardian paradigm extends a subversive challenge to all the grand narratives that have been enjoying the status of an enviable complete code of life in the world for a long time. Even a cursory scan over the play under analysis creates a strong feel that Beckett very smartly, comprehensively and successfully questioned the relevance of the totalizing metanarratives to the present times. Being an imaginative writer, he was well aware of the fact that ridicule is a much more useful weapon than satire in the context of political literature. There are so many foundationalist ideologies that he ridicules in his dramatic writing. Christianity as a religion is well exposed; the gravity of philosophy is devalued; the traditional luxury that the humans get from the art of poetry is ruptured and the great ideals of struggle are punctured. He achieves his artistic and ideologically evolved authorial intentions with a ringing success. It is interesting to note that he maintains a healthy balance between art and message. Keyswords: Beckett, Lyotard, The Postmodern Condition, Waiting for Godot Pages: 85-94 Article: 8 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)08 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)08 Download Pdf: download pdf view article Creative Commons License Effect of Parenting Styles on Students‚Äô Academic Achievement at Elementary Level Authors: Hafsa Noreen Mushtaq Ahmad Uzma Shahzadi Abstract: The study intended to find out the effect of parenting styles on students‚Äô academic achievement. Current study was quantitative in nature. All elementary level enrolled students at government schools in the province of the Punjab made the population of the study. Multistage sampling was used to select the sample from four districts of one division (Sargodha) of the Punjab province i.e., Sargodha. A sample size i.e., n=960; students and their parents were participated in this study. Research scales i.e. Parenting Styles Dimension Questionnaire (PSDQ) was adapted to analyze and measure parents‚Äô parenting styles and an achievement test was developed to measure the academic achievement of the elementary students. After pilot testing, reliability coefficient Cronbach Alpha values for PSDQ and achievement test were 0.67 and 0.71 Data was collected and analyzed using frequencies count, percentages, mean scores and one way ANOVA. Major findings of the study were; Majority of the parents had authoritative parental style, a handsome number of parents keep connection of warmth and support with their children, show intimacy, focus on discipline, do not grant autonomy to their children, do not indulge with their children and as well as a handsome number of students were confident during their studies and study, further, found that parental style had positive relationship with academic achievement. Recommendations were made on the basis of findings and conclusion such as arrangement of Parents Teachers Meetings (PTM‚Äòs), parents‚Äô training, provision of incentives and facilities to motivate families might be an inclusive component of elementary education program. Keyswords: Academic Achievement, Elementary Education, Parenting Styles Pages: 95-110 Article: 9 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)09 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)09 Download Pdf: download pdf view article Creative Commons License Kashmir Conflict and the Question of Self-Determination Authors: Izzat Raazia Saqib Ur Rehman Abstract: The objective of this paper is to explore relations between Pakistan and India since their inception in the perspective of Kashmir conundrum and its impact on the regional security. Kashmir is the unfinished agenda of partition and a stumbling block in the bilateral relations between Pakistan and India. After the partition of sub-continent in 1947, Pakistan and India got their sovereign status. Kashmir conflict, a disputed status state, is the byproduct of partition. Pakistan and India are traditional arch-foes. Any clash between Pakistan and India can bring the two nuclear states toe-to-toe and accelerate into nuclear warfare. Due to the revulsion, hostility and lack of trust between the two, the peaceful resolution of the Kashmir issue has been long overdue. Ever-increasing border spats, arms race and threat of terrorism between the two have augmented anxiety in the subcontinent along with the halt of talks between India and Pakistan at several times. Additionally, it hampers the economic and trade ties between the two. India, time and again, backtracked on Kashmir issue despite UN efforts to resolve the issue. Recently, Indian government has responded heavy-handedly to the Kashmiri agitators‚Äô demand for sovereignty and revocation of ‚ÄòSpecial Status‚Äô of Kashmir impacting the stability of the region in future. Keyswords: India, Kashmir Conundrum, Pakistan, Regional Security, Sovereignty Pages: 111-119 Article: 10 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)10 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)10 Download Pdf: download pdf view article Creative Commons License Exploring Image of China in the Diplomatic Discourse: A Critical Discourse Analysis Authors: Muhammad Afzaal Muhammad Ilyas Chishti Abstract: The present study hinges on the major objective of analyzing Pakistani and Indian diplomatic discourses employed in portrayal of image of China. Data comprises the official discourse which is used in diplomatic affairs of both the states. The extensive investigation seeks insights from the fundamentals of Critical Discourse Analysis propounded by van Dijk, Fairclough and Wodak with a special focus on Bhatia‚Äôs (2006) work. The study reveals that the image of China has always been accorded priority within Indian and Pakistani diplomatic discourse even though nature of bilateral relations among China, India and Pakistan is based on entirely different dynamics; Indian and Pakistani diplomatic discourses are reflective of sensitivities involved within the bilateral relations. Through employment of linguistic techniques of ‚Äòpositivity‚Äô, ‚Äòevasion‚Äô and ‚Äòinfluence and power‚Äô, Indian diplomats have managed not to compromise over the fundamentals in bilateral relations with China despite Pakistan‚Äôs already strengthened and deep-rooted relations with China. While Pakistani diplomatic fronts have been equally successful in further deepening their already strengthened relations in the midst of surging controversies on CPEC, BRI and OBOR. Hence, diplomatic fronts of both the counties, through employment of ideologically loaded linguistic choices, leave no stone unturned in consolidation of the diplomatic relations with China. Keyswords: CDA, China Image, Corpus, Language of Diplomacy, Political Discourse Analysis Pages: 120-133 Article: 11 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)11 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)11 Download Pdf: download pdf view article Creative Commons License Students‚Äô Perception about Academic Advising Satisfaction at Higher Education Level Authors: Rukhsana Sardar Zarina Akhtar Shamsa Aziz Abstract: The purpose of the study was to examine the students‚Äô perception about academic advising satisfaction at higher education level. All the students from two years master (M.A) degree programme and four years (BS) degree programme of eight departments from International Islamic University Islamabad (IIUI), Faculty of Social Sciences were taken as a population of the study. 475 students were randomly selected as a sample of the study. The Academic Advising Inventory (AAI) was used to assess Academic Advising Style. For measuring level of the satisfaction, descriptive statistics was used. To compare the mean difference department-wise and gender-wise about academic advising satisfaction t.test was applied. It was concluded that from the major findings of the study those students who received departmental academic advising style are more satisfied as compared to those students who provided prescriptive academic advising style. Female students seemed more satisfied as compared to male students regarding the academic advising style provided to them. Students who satisfied from developmental academic advising style and they were also highly satisfied from the advising provided to them at Personalizing Education (PE) and this is the subscale of developmental academic advising whereas students who received prescriptive academic advising they were also satisfied from the advising provided to them regarding personalizing education and academic decision making but their percentage is less. It is recommended to Universities Administration to focus on Developmental Academic Advising Style and establish centers at universities/department level and nominate staff who may be responsible to provide developmental academic advising. Keyswords: Academic Advising, Higher Level, Students‚Äô Perception Pages: 134-144 Article: 12 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)12 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)12 Download Pdf: download pdf view article Creative Commons License Perceptions of Sexual Harassment in Higher Education Institutions: A Gender Analysis Authors: Ruhina Ghassan Dr. Subha Malik Nayab Javed Abstract: Sexual harassment is a social issue which is present in every society, globally, which interferes in an individual‚Äôs social and professional life. It happens almost everywhere i.e. at workplaces, public places or institutes as well. The focus of the present study was to explore the differences of male and female students‚Äô perception of sexual harassment. This study was a quantitative research. Sample of the study included of 400 students (200 males and 200 females) from two government and two private universities. In the present study, Sexual Harassment Perception Questionnaire (SHPQ) was used to find out these differences in perceptions as every person has his own view for different situations. The study revealed the significant differences in perception of students. Study showed that both genders perceived that female students get more harassed than male students. The factors that affect the perception frequently were gender and age. The findings recommended that regulations for sexual harassment should be implemented in universities; laws should be made for sexual harassment in higher education institutes. Students should be aware of sexual harassment through seminars, self-defense classes and awareness campaigns. And every institute should have a counseling center for the better mental health of students. Keyswords: Gender Differences, Higher Educational Institutions, Sexual Harassment Pages: 145-158 Article: 13 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)13 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)13 Download Pdf: download pdf view article Creative Commons License Role of IMF Over the Governance Structure and Economic Development of Pakistan Authors: Ali Qamar Sheikh Dr. Muhammad Imran Pasha Muhammad Shakeel Ahmad Siddiqui Abstract: Developing countries like Pakistan seeks for financial assistance in order to fulfil their deficits. IMF is one of the largest financial institution who give loans to countries who need it. This research has studied the IMF role and the effects of IMF conditions on the economy of Pakistan. To carry out this research, both quantitative data from primary sources has been gathered and qualitative analysis has been made to signify whither this borrowing creating and maintaining dependency of Pakistan on West and financial and governance structure constructed to curtail Countries like Pakistan. The results concluded that there is negative and insignificant relationship between GDP and IMF loans in the long run. The short-term dynamic shows that weak economic and Political Institutions in Pakistan. The Development dilemma constitutes dependency even today. The Current Budget Deficit Pakistan&apos;s fiscal deficit climbs to Rs 3.403 trillion in 2020-21 needs to be readdressed in such a manner that Pakistan can counter Balance of Payments and import/export imbalance. Keyswords: Dependency, Development, IMF, Loans, Debt, Pakistan, Governance structure Pages: 159-172 Article: 14 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)14 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)14 Download Pdf: download pdf view article Creative Commons License Climate Change and the Indus Basin: Prospects of Cooperation between India and Pakistan Authors: Sarah Saeed Prof. Dr. Rana Eijaz Ahmad Abstract: Climate change is transforming the global societies. The shift in average temperature is putting negative impacts on human health, food production and the natural resources. In the wake of the altered climate, water flow in the river systems is experiencing variability and uncertainty. This paper aims at studying the negative impacts of climate change on the water resources of the Indus Basin and investigate the prospects of cooperation between India and Pakistan; two major riparian nations sharing the basin. Adopting the case study approach, a theoretical framework has been built on the ‚ÄòTheory of the International Regimes‚Äô. It has been argued that institutional capacity and the dispute resolution mechanism provided in any water sharing agreement determine the extent of cooperation among the member states. Since India and Pakistan are bound by the provisions of the Indus Waters Treaty, this study tries to assess the effectiveness of this agreement in managing the negative consequences of the climate change. Keyswords: Climate Change, Cooperation, Dispute Resolution Mechanism, Institutional Capacity Pages: 173-185 Article: 15 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)15 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)15 Download Pdf: download pdf view article Creative Commons License Translation, Cultural Adaptation and Validation of Behavioral-Emotional Reactivity Index for Adolescents Authors: Saima Saeed Farah Malik Suzanne Bartle Haring Abstract: Measuring differentiation of self in terms of behavioral/emotional reactivity towards parents is important because of the complex parent-child connection. This needs a valid and reliable measure to assess the differentiation of self particularly in a relationship with parents. Behavior\Emotional Reactivity Index is such a tool that fulfills this purpose. The present study was carried out to culturaly adapt and translate BERI into the Urdu language and establish the psychometric properties of Urdu version. A sample of 303 adolescents of age (M = 16.07, SD = 1.77) was taken from different schools and colleges. Scale was split into Mother and father forms for the convenience of respondents. Findings supported the original factor structure of the BERI-original version. Higher-order factor analysis showed good fit indices with excellent alpha ranges (Œ±= .91 to Œ±=.80). BERI scores were compared for the adolescents who were securely attached with parents and insecurely attached with parents which showed a significant difference between the groups. BERI-Urdu version was found to be a valid and reliable measure in the Pakistani cultural context which gives researchers new directions to work with adolescents. Keyswords: Adolescence, Differentiation of Self, Behavioral, Emotional Reactivit, Index, Parental Attachment Pages: 186-200 Article: 16 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)16 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)16 Download Pdf: download pdf view article Creative Commons License Notion of Repression in Modern Society: A Comparative Analysis of Sigmund Freud and Herbert Marcuse Authors: Khadija Naz Abstract: One of the fundamental issues for modern civilized man is how to adapt a modern society without losing his individual status. Is it possible for an individual to adjust in a society where he/she loses his/her individuality and becomes part of collectivity? One point of view is that for society to flourish, man needs to be repressed. But to what extent is repression necessary for societies to rise and survive? This paper shall examine the above given questions from the standpoint of two thinkers who greatly influenced twentieth-century thought: Sigmund Freud and Herbert Marcuse. To undertake this task, first the term Repression shall be examined and then the notions of Freud and Marcuse will be discussed to determine the degree of repression required for the development of modern society. Keyswords: Modern Society, Performance Principle, Repression, Surplus-Repression, The Pleasure Principle, The Reality Principle Pages: 201-214 Article: 17 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)17 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)17 Download Pdf: download pdf view article Creative Commons License Perceptions of Teacher Educators about Integration of (ESD) in Elementary Teachers Education Program Authors: Dr. Rukhsana Durrani Dr. Fazal ur Rahman Dr. Shaista Anjum Abstract: Education and sustainable development have a close relationship as education provides sustainability to society. This study explored the perceptions of teacher educators for integration of Education for Sustainable Development (ESD) in B.Ed. 4 years‚Äô elementary program. Four major components of ESD i.e., Education, Social &amp; Culture, Economic and Environment were included in study. 127 teacher educators from departments of education were randomly selected from public universities of Pakistan who were offering B.Ed. 4 years‚Äô elementary program. Data was collected through questionnaires from teacher educators. The findings recommended the inclusion of the components of Education for Sustainable Development (ESD) in curriculum of B.Ed. 4 years‚Äô elementary program. Keyswords: B.Ed. 4 Years Elementary Curriculum, Sustainable Development, Integration, Teacher Education Pages: 215-225 Article: 18 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)18 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)18 Download Pdf: download pdf view article Creative Commons License Exploring TPACK skills of prospective teachers and challenges faced in digital technology integration in Pakistan Authors: Tariq Saleem Ghayyur Dr. Nargis Abbas Mirza Abstract: The current study was aimed to explore TPACK skills of prospective teachers and challenges faced in digital technology integration in Pakistan. The study was qualitative in nature and semi structured interview schedule was developed to collect data from prospective teachers. Purposive sampling technique was employed to collect data from 20 prospective teachers of 7 public sector universities. It was concluded that majority of the prospective teachers used general technological and pedagogical practices (GTPP), technological knowledge practices (TKP), Technological Pedagogical Knowledge practices (TPKP), Technological Content Knowledge practices (TCKP). Majority of prospective teachers reported multiple challenges in integration of digital technology in teacher education programs including lack of teacher training as one of the largest hurdle in digital technology integration, lack of digital technology resources or outdated digital technology resources, inadequate computer lab, lack of learning apps (courseware), financial constraints, lack of teachers‚Äô motivation to use digital technology, slow computers available at computer labs, and unavailability of technical support. It was recommended that digital technology infrastructure should be improved across all teacher education institution and it was further recommended that TPACK model of digital technology integration should serve digital technology integration in teacher education programs in Pakistan. Keyswords: Challenges, Digital Technology Integration, Digital Technology Resources, Digital Technology, TPACK Pages: 226-241 Article: 19 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)19 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)19 Download Pdf: download pdf view article Creative Commons License Revisiting the Linkage between Money Supply and Income: A Simultaneous Equation Model for Pakistan Authors: Zenab Faizullah Dr. Shahid Ali Muhammad Imad Khan Abstract: A reliable estimate of the money supply is an important sign of the Gross Domestic Product (GDP) and many other macroeconomic indicators. It is widely discussed that over a long period of time, there is a strong link between GDP and money supply. This link is significantly important for formation of monetary policy. The main aim of this study is to estimate the income-money supply model for Pakistan. This study estimates the income-money supply model for Pakistan over the period of 2009 to 2019. The study uses Two Stage Least Square (2SLS) econometric technique due to the presence of endogeneity problem in the model under consideration. The existence of simultaneity between money supply (M2) and income (GDP) is also clear from the results of Hausman Specification test for simultaneity between M2 and GDP. The results further show that there exists a strong money-income relationship in case of Pakistan. Keyswords: Money Supply, Income, Simultaneous Equations Pages: 242-247 Article: 20 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)20 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)20 Download Pdf: download pdf view article Creative Commons License Analyzing the Mechanism of Language Learning Process by the Use of Language Learning Strategies Authors: Shafiq Ahmad Farooqi Dr. Muhammad Shakir Sher Muhammad Awan Abstract: This analytical research study involves the use of learning strategies to know the mechanism of learning a second language. People acquire their native language (L1) without any conscious effort and they have a complete knowledge of L1 and are competent in their native language even without going to school. It is believed that language learning is a process as well as an outcome and the focus of current study is to understand the process of learning a second language. The population in this study comprised of 182 boys and Girls Govt. Higher Secondary Schools studying at intermediate level in the 11 Districts of the Southern Punjab. The sample was selected through random probability sampling and consisted of 40 subject specialists teaching the subject of English in Govt. higher secondary schools with 400 students studying English at Intermediate level. A questionnaire comprising some common and easily accessible learning strategies was designed to determine the frequency of these strategies used in the classrooms by the language learners through the specialists of the subject. The data was collected from the selected sample through the subject specialists teaching in these schools. The data was collected quantitatively and was analyzed in the statistical package for social sciences (SPSS) version 20. The most common 27 language learning strategies (LLS) were applied to analyze the process of language learning. In the light of the results of the study, it was concluded that application of the learning strategies according to the nature of the text is helpful in understanding the language functions and its application. Keyswords: Language Acquisition, Learning Strategies, Mechanism of Language Learning Pages: 249-258 Article: 21 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)21 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)21 Download Pdf: download pdf view article Creative Commons License Secondary School Science Teachers‚Äô Practices for the Development of Critical Thinking Skills: An Observational Study Authors: Dr. Muhammad Jamil Dr. Yaar Muhammad Dr. Naima Qureshi Abstract: In the National curriculum policy documents, to produce rationale and independent critical thinkers, different pedagogical practices have been recommended like cooperative learning, questioning, discussion, etc. This qualitative case study aimed at analyzing secondary school science teachers‚Äô practices for the development of critical thinking skills in secondary school students. There were twelve classrooms (four from each subject of Physics, Chemistry and Biology) selected as cases. Video recording was used for the observations for six lessons in each classroom. In this way, a total of 72 observations were conducted lasting for approximately 35 minutes. Qualitative content analysis was used for data analysis through Nvivo 12. The findings of the observations revealed that all the teachers used the lecture method. They used this to cover the content at a given specific time. There was not much focus on the development of critical thinking. In a few of the classrooms, the students were engaged and active during learning different specific topics. Whiteboard was used as a visual aid by most of the teachers. Furthermore, to some extent, discussion, questioning, and daily life examples were used in different classrooms. It is recommended that teachers‚Äô professional development should be conducted to focus on the development of critical thinking skills through pedagogical practices which have been recommended by the national education policy documents. Keyswords: Analysis, Critical Thinking, Curriculum Policy, Pedagogy, Secondary Level Pages: 259-265 Article: 22 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)22 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)22 Download Pdf: download pdf view article Creative Commons License Historical Development of Clinical Psychology in Pakistan: A Critical Review-based Study Authors: Muhammad Nawaz Shahzad Dr. Mushtaq Ahmad Dr. Muhammad Waseem Tufail Abstract: Clinical Psychology is clinical and curing psychological practices in Pakistan. The present research study endeavors to examine the contemporary status of Clinical Psychology in the country and descriptively analyzes the significant contribution of various psychologists in its development. The study also elaborates the emergence of Clinical Psychology and its treatment aspects in the country. The experimental approach of the treatment psychology has also been defined. The role of different scholars to set and promote the Clinical Psychology as discipline and dealing about treatment of Human mind has also been discussed here. The study also presented the scenario of the issues of legislative acknowledgment, qualifications mandatory for practice, communal awareness of cerebral treatment, the tradition of ethnic and native practices about the clinical psychological treatments has also been discussed. Keyswords: Approaches, Clinical Psychology, Psychologist, Therapist Pages: 266-272 Article: 23 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)23 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)23 Download Pdf: download pdf view article Creative Commons License Impact of Devolution of Power on School Education Performance in Sindh after 18th Constitutional Amendment Authors: Abdul Hafeez Dr. Saima Iqbal Muhammad Imran Abstract: Devolution of the authority from central units of empowering authorities to the local level to develop and exercise policies at local or organizational level is under debate in various countries of the world. The legation in with the name of 18th constitutional amendment in constitution of 1973 of Pakistan ensures more autonomy to federal units. The difference between province and federation mostly creates misunderstanding in the belief of cooperation and universalism of education standards, expenditures and service delivery. Very currently the ministry of education and local government encoring principles and headmasters to adopt self-management skills to be updated to accept the spin of power from higher authorities to lower authorities‚Äô pedagogical and local schools. In this qualitative research semi structured questioner were incorporated as data collection tool equally, the data was analyzed by usage of NVivo software. In this regard Government of Sindh has introduced various reforms and new trends like objectives and policy pillars, better government schools, improved learning outcomes and increased and improved funding in the education sector Sindh government has so far been unable to effectively use its resources to implement effective governance system which provides quality and sustained education in the province. To achieve this basic universal education, equally fourth objective of Sustainable Development Goal (SDG) the educational leaders must develop a comparative education setup that help to educate planers to plan and design standards for school leaders, instruction, appropriate professional development of teachers, ways to support school leaders to change in mission. Parallel, develop new program for early childhood, school and class size and ensure school enrollment. Keyswords: 18th Constitutional Amendment, Devolution of Power, Sindh Education Performance Pages: 273-285 Article: 24 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)24 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)24 Download Pdf: download pdf view article Creative Commons License Legal Aspects of Evidence Collected by Modern Devices: A Case Study Authors: Muhammad Hassan Zia Alvina Ali Abstract: This paper is a qualitative research of different case laws dealing with modern technological evidence. Courts were required to adopt new methods, techniques and devices obtained through advancement of science without affecting the original intention of law. Because of modern technology, a benefit could be taken from said technology to preserve evidences and to assist proceedings of the Court in the dispensation of justice in modern times. Owing to the scientific and technological advancements the admissibility of audio and visual proofs has grown doubtful. No doubt modern evidence assist the court in reaching out to the just decision but at the same time certain criteria need to be laid down which must be satisfied to consider such evidence admissible. Different Case laws are discussed here to show how the cases were resolved on the basis of technological evidence and when and why such evidence have been rejected by the court, if it did. Moreover, legal practices developed in various countries allow our Courts to record evidence through video conferencing. The Honorable Supreme Court of Pakistan directed that in appropriate cases statement of juvenile rape victims and other cases of sensitive nature must be recorded through video conferencing to avoid inconvenience for them to come to the Court. Nevertheless, it has some problems. The most important among them is the identification of the witness and an assurance that he is not being prompted when his statement is recorded. In this paper protocols that are necessary to follow while examining witness through video link are discussed Keyswords: DNA Profiling, Finger Prints, , Telephone Calls, Video Tape Pages: 286-297 Article: 25 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)25 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)25 Download Pdf: download pdf view article Creative Commons License The Political Economy of Terrorisms: Economic Cost of War on Terror for Pakistan Authors: Muhammad Shakeel Ahmad Siddiqui Dr. Muhammad Imran Pasha Saira Akram Abstract: Terrorism and its effect on contemporary society is one of the core and vital subjects of International Political Economy (IPE) during the last years. Despite the fact that this is not a new phenomenon, special attention has been given to this issue, specifically after the terrorist attacks of 9/11, 2001. The objective of this paper analyzes to what dimensions terrorism affects the global economy mainly the two predominant actors of the conflict i.e. Pakistan and the United States. For this purpose, this article will take a look at the financial cost of War for Pakistan and how Pakistan‚Äôs decision to become frontline State has affected its Economy, its effect on agriculture, manufacturing, tourism, FDI, increased defense costs The normative and qualitative methodology shows a significant disadvantage between terrorist activities and economic growth, social progress, and political development. The results shows that Pakistan has bear slow economic growth while facing terrorist activities more than US. In this last section, the paper suggests ways and means to satisfy people around the world not to go in the hands of fundamentals and terrorists. Keyswords: Cost of War, Economic Growth, Frontline States, Pak Us Relations, Terrorism Pages: 297-309 Article: 26 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)26 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)26 Download Pdf: download pdf view article Creative Commons License A Comparative Study of Grade 10 English Textbooks of Sindh Textbook Board and Cambridge ‚ÄúO Level‚Äù in the perspective of Revised Bloom‚Äôs Taxonomy Authors: Mahnoor Shaikh Dr. Shumaila Memon Abstract: The present study evaluated the cognitive levels of reading comprehension questions present in grade 10 English Textbooks namely English Textbook for grade 10 by Sindh Textbook Board and compared it to Oxford Progressive English book 10 used in Cambridge ‚ÄúO Level‚Äù in the perspective of Revised Bloom‚Äôs Taxonomy. Qualitative content analysis was used as a methodology to carry out the study. To collect the data, a checklist based on Revised Bloom‚Äôs taxonomy was used as an instrument. A total of 260 reading comprehension questions from both the textbooks were evaluated. The findings of the study revealed that reading comprehension questions in English textbook for grade 10 were solely based on remembering level (100%) whereas the questions in Oxford Progressive English 10 were mainly based on understanding level (75.5%) with a small percentage of remembering (12.5%), analyzing (11.1%) and evaluating level (0.74%). This suggests that the reading comprehension questions in both the textbooks are dominantly based on lower-order thinking skills. Keyswords: Bloom‚Äôs Taxonomy, Content Analysis, Reading Comprehension, Textbook Evaluation Pages: 310-320 Article: 27 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)27 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)27 Download Pdf: download pdf view article Creative Commons License Assessing the Preparedness of Government Hospitals: A Case of Quetta City, Balochiatan Authors: Sahar Arshad Syed Ainuddin Jamal ud din Abstract: Earthquake with high magnitude is often resulting in massive destruction with more causalities and high mortality rate. Timely providence of critical healthcare facilities to affected people during an emergency response is the core principle of disaster resilient communities. The main objective of this paper is assessing the hospital preparedness of government hospitals in Quetta. Primary data was collected through questionnaire survey. Total of 165 sample size chosen via simple random sampling. Relative important index (RII) is used to analyze the overall situation of hospitals preparedness in term of earthquake disaster. Findings of the study showed that the preparedness level of government hospitals in Quetta is weak to moderate level. Based on the findings this study recommends the necessary measures to minimize the risk of earthquake disaster including training and exercise programs for the staff of hospital, proper resource management to efficiently use the existing machinery and equipment in the meeting of disaster to enhance employee‚Äôs performance and preparedness of government hospitals in Quetta to deal with earthquake disaster. Keyswords: Earthquake, Preparedness, Relative Important Index Pages: 321-329 Article: 28 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)28 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)28 Download Pdf: download pdf view article Creative Commons License Development of Reasoning Skills among Prospective Teachers through Cognitive Acceleration Approach Authors: Memoona Bibi Dr. Shamsa Aziz Abstract: The main objectives of this study were to; investigate the effects of the Cognitive Acceleration approach on the reasoning skills of the prospective teachers at the university level and compare the effects of the Cognitive Acceleration approach and traditional approach concerning reasoning skills of prospective teachers‚Äô at the university level. The study was experimental and followed a pre-test post-test control group experimental design. The sample of the study included the experimental group and control group from the BS Education program in the Department of Education at International Islamic University Islamabad. A simple random sampling technique was used to select the sample after pre-test and pairing of prospective teachers. CTSR (classroom test for scientific reasoning) developed by A.E. Lawson (2000) was used to collect the data through pre-tests and post-tests. The experimental group‚Äôs perception about different activities of the experiment was taken through a self-made rating scale. Collected data were analyzed by calculating mean scores and t-test for hypothesis testing by using SPSS. The main findings of the study revealed that the Cognitive Acceleration teaching approach has a significant positive effect on the reasoning skills development of prospective teachers at the university level. Findings also showed that participants found this teaching approach effective and learned many new concepts and skills with the help of thinking activities. Based on findings it has been concluded that the Cognitive Acceleration teaching approach might be encouraged for training prospective teachers at the university level and training sessions about the use of the Cognitive Acceleration approach must be arranged by teacher education programs and institutions. Keyswords: Cognitive Acceleration Approach, Prospective Teachers, Reasoning Skills, Traditional Approach Pages: 330-342 Article: 29 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)29 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)29 Download Pdf: download pdf view article Creative Commons License Spatial Injustice in Shamsie‚Äôs Kartography Authors: Syeda Hibba Zainab Zaidi Dr. Ali Usman Saleem Sadia Waheed Abstract: Social space under postmodernism and wave of globalization have suffered in and its idealistic representations are lost and deteriorated which ultimately led to discursiveness in the lives of postmodern man, especially Karachiites. The boundaries of geographies play a significant role in shaping fates, biographies, social superstructures and shared collective histories of its residents. Considering this, Henri Lefebvre and Edward William Soja, argue that space is something which determines the living circumstances within the particular social framework and instigates and controls various societal happenings. City space of Karachi suffers from appalling distortions as a part of postmodern, globalized and capitalist world. By employing Lefebvre‚Äôs idea of spatial triad and Soja‚Äôs views of the trialectrics of spaciality, this paper foregrounds how social space enforces spatial injustice and serves for the inculcation of spatial cleansing in the lives of inhabitants of urban space. Using Shamsie‚Äôs Kartography as an interpretive tool for contemporary urban environment, this paper inquires the engrafting of spatial cleansing in the lives of Karachiites resulting in multiple standardization and segregation on the basis of living standards among different social strata. This research substantiates how in Kartography, Materialism nibbles the roots of social values and norms while sequentially administering Spatial Injustice in the lives of Karachiites. This paper proclaims the scarcity of execution of Spatial Justice in the lives of common people in this postmodern globalized capitalist era. This paper urges the possibility of a utopian urban space with enforced spatial justice where people can be saved from dilemmas of injustice and segregation, especially Karachiites. Keyswords: Capitalistic Hegemony, City Space, Globalization, Spatial Cleansing, Spatial Injustice Pages: 343-352 Article: 30 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)30 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)30 Download Pdf: download pdf view article Creative Commons License A Quasi-Experimental Study on the Performance and Attitudes of Pakistani Undergraduate Students towards Hello English Language Learning Application Authors: Wafa Pirzada Dr. Shumaila Memon Dr. Habibullah Pathan Abstract: With the advancement of technology, more and more avenues of bringing creativity and innovation in language learning have opened up. These exciting advances have given rise to a new field of study within linguistics, termed Mobile Assisted Language Learning (MALL). This paper aims to fill the gap of MALL research in the area of grammar teaching in the Pakistan. Two BS Part 1 classes from University of Sindh, Jamshoro, were chosen for this quasi-experimental study. In total, 62 out of 101 students volunteered to use the Hello English application for 2 months, making up the experiment group, and the remaining 39 students were put in a control group. Paired Samples T-Test was run on pretest and posttest results which revealed no significant difference in both groups‚Äô performances, proving that Hello English application could not significantly improve students‚Äô grammar performance. However, in spite of the lack of a significant difference between the test results, the data gathered through the attitudinal survey showed that students still found mobile application very easy to use and effective in language learning. Keyswords: Attitudes, Grammar Learning, Hello English, Mobile Language Learning, Technology In Language Learning Pages: 353-367 Article: 31 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)31 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)31 Download Pdf: download pdf view article Creative Commons License Impact of Determinants on the Profile Elevation of Secondary School Teachers in Pakistan Authors: Zahida Aziz Sial Dr. Farah Latif Naz Humaira Saadia Abstract: The foremost purpose of this research paper was to interrogate the effects of determinants on the educational and social profile of secondary school teachers in Pakistan. The key question taken was related to determinants that affect teachers‚Äô profile. The Population of the study was secondary school teachers of Punjab province. A questionnaire was used as research instrument. The researcher personally visited the schools to administer the questionnaire. E-Views software was used for data analysis. Moreover, OLS regression model and LOGIT regression model were carried out. It was found that the variable years of teaching experience (EXPYR) (*** 0.03) can have a vital concrete effect upon the societal figuration of teachers as the experience of teachers grows, so does their social interactions with officials, colleagues, students and friends increases. The said variable is significant at 10 percent level. The variable, Residence (RESIDE) (** 0.53) have a significant impact upon civic links. This obviously associated with less community connection of country side teachers than the teachers residing in urban areas. Keyswords: Determinants, Elevation, Educational Profile, Social Profile, Secondary School Teacher Pages: 368-372 Article: 32 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)32 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)32 Download Pdf: download pdf view article Creative Commons License Impact of War on Terror on the Tourism Industry in Swat, Pakistan Authors: Sabir Ihsan Prof. Dr. Anwar Alam Aman Ullah Abstract: The present study was designed to ascertain the status of tourism before insurgency, during insurgency and after insurgency in District Swat-KP Pakistan. The study is quantitative and descriptive in nature. A diverse sample size of 370 out of 9014 was selected through convenient sampling strategy. Notwithstanding, the objectives of the study was achieved through structured questionnaire. Data was analysed through chi-square at Bi Variate level. Findings of the study revealed that earning livelihood in swat was significantly associated (P=0.016), (P=0.003) with tourism industry prior 2009 and present time respective, but the same statement was observed non-significant (P=0.075) at the time of insurgency. Arranging different festivals in the study area and establishment of different showrooms for local handcrafts, artificial jewellery and woollen shawl are some of the recommendations of the study. Keyswords: Business, Insurgency, Swat, Tourism Pages: 373-385 Article: 33 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)33 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)33 Download Pdf: download pdf view article Creative Commons License Challenges and Prospects of Pak-China Economic Corridor Authors: Muhammad Mudabbir Malik Prof. Dr. Muqarrab Akbar Abstract: Pak-China has historic relationships from the emergence of both states, and were proved long-lasting in every thick and thin times. In initial times they supported each other in foreign policies and regional issues. Pakistan and China have border disputes with India, which forced them to come close to counter India, letter on the economic interests strengthened these relations. In order to maximize the economic benefits, China announced economic corridor with the name China Pakistan Economic Corridor (CEPC). It was thought it will boost the economic growth of China, and as a prime partner Pakistan will also get economic benefits. In order to completely understand how Pakistan and China came on the same page and decided to put CPEC into reality we have to understand the Geo-political Importance of Pakistan, Strategic and economic importance of CPEC for China and Pakistan, Influence and concerns of West and neighboring countries including India. Domestic limitations and all the possible benefits and risks involved in this project for both Pakistan and China, this research acknowledges all these questions. Keyswords: Challenges, China, CPEC, Domestic Limitations Economic Growth, Pakistan, Western and Regional Concerns Pages: 386-404 Article: 34 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)34 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)34 Download Pdf: download pdf view article Creative Commons License An Analysis of Learning Practices and Habits of Children at Early Childhood Education: Students‚Äô Perspective Authors: Masood Ahmad Sabiha Iqbal Shaista Noreen Abstract: The study was designed to analysis learning practices and habits of children at early childhood education. The major objective of the study was to find out the learning practices and habits of children. Problem was related to current situation, so survey method was exercised, 220 students were selected with the help of convenient sampling technique. Self-constructed questionnaire were exercised. The collected data was analyzed and calculate frequency, percentage, mean score, standard deviation and t-test of independent variable. The major findings of the study were; students learn from the pictures, cartoons and funny face; student‚Äôs eyes get tired of reading. When student read context continuously then they feel that their eyes get tired. There was a significance difference between male and female student about learning practices and habits of children. Keyswords: Early Childhood Education, Learning Practices and Habits, Pre-School Students Pages: 405-416 Article: 35 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)35 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)35 Download Pdf: download pdf view article Creative Commons License Gender Identity Construction in Akhtar‚Äôs Melody of a Tear Authors: Dr. Amna Saeed Hina Quddus Abstract: This study aims to discuss the notion of gender in terms of performativity and social construction. It also draws upon the idea of gender identity construction and how it relates to the society, performativity and biology. As its theoretical framework, the study relies upon the Performative Theory of Gender and Sex (1990) presented by Judith Butler and studies the gender identity construction in the female protagonist of Akhtar‚Äôs Melody of a Tear. Zara is a girl who is raised as a boy from his father and there is a kind of dilemma in Zara‚Äôs personality related to being masculine and feminine. The cultural norms of a particular gender are also a cause of this dilemma. Throughout the novel, she is in a conflicting state whether she should behave feminine or masculine. She is being depicted as an incomplete person until she finds and resolves this issue of gender identity. The paper discusses the gender performativity, social construction, cultural norms and identity as these are all contributing to the confusion and construction of the protagonist‚Äôs identity. Character analysis is used as the methodology of analysis. Keyswords: Cultural Norms, Femininity And Identity Confusion, Gender, Performativity, Masculinity, Social Construction Pages: 417-427 Article: 36 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)36 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)36 Download Pdf: download pdf view article Creative Commons License The Level of Impulsivity and Aggression among Crystal Meth and Cannabis Users Authors: Dr. Umbreen Khizar Muhammad Shafique Sana Nawab Abstract: Cannabis and crystal meth use is pervading in our society. Present study was conducted to explore the relationship between level of impulsivity and aggression among crystal meth and cannabis users. The sample of the present study was comprised of 100 participants. There were 50 cannabis and 50 crystal meth users who were diagnosed on the basis of DSM-V without any comorbidity. The sample were taken from all age range of population. The minimum education level was primary and maximum education level was graduation and above. The sample was selected from different drug rehabilitation centers of Rawalpindi and Islamabad, Pakistan. Demographic Performa was used to collect the initial important information, The ‚ÄúBarratt Impulsiveness Scale was used to measure the impulsivity and ‚ÄúAggression Questionnaire‚Äù were used to measure the level of aggression. Finding of the study showed that there are significant differences among crystal meth and cannabis users on level of aggression. The calculated mean value for crystal meth user and for cannabis users indicates that crystal meth users have higher level of aggression as compared to the cannabis user. Over all analysis indicates a significant positive correlation of impulsivity with the variable aggression. The alpha coefficient value for all scale is acceptable. Keyswords: Aggression, Cannabis Users, Crystal Meth, Impulsivity Pages: 428-439 Article: 37 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)37 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)37 Download Pdf: download pdf view article Creative Commons License Impact of Social Factors on the Status of Tribal Women: A Case Study of the (Erstwhile) Mohmand Agency Authors: Sadia Jabeen Prof. Dr. Anwar Alam Muhammad Jawad Abstract: This study investigates the impact of socio-economic and cultural factors on the status of tribal women in the erstwhile Mohmand agency of the Ex-Federally Administered Tribal Area (FATA), Pakistan. Cultural practices and illiteracy impede the role of women in socio-economic development. The respondents were randomly selected from tehsil Ekka Ghund and Pindialai with a sample size of 370, through stratified random sampling. Data collected through structured interview schedule, FGD and observation technique. The study reveals that tribal practices early marriages, joint family system, tradition of forced marriages, compensation/Swara, exchange, purchase marriages, hampers women‚Äôs socioeconomic status. The illiteracy rate is high among the tribal women and it further undermines their role and negatively affects their socio-economic status. However, improvement in women status needs peace and stability, reforms in the constitution for women empowerment and active participation, improvement in the quality and quantity of education, women employability, skills development and women entrepreneurship Keyswords: Empowerment and Education, Marriage Types, Tribal Women Role, Tribal Women Status, Violence against Women Pages: 440-455 Article: 38 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)38 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)38 Download Pdf: download pdf view article Creative Commons License Effects of Heavy School Bags on Students‚Äô Health at Primary Level in District Haveli (Kahutta) Azad Jammu and Kashmir Authors: Dr. Muhammad Mushtaq Shamsa Rathore Mishbah Saba Abstract: Heavy school bags is a very serious issue for the health of the primary level students throughout the world particularly in Azad Jammu and Kashmir. This study intends to explore the effect of heavy school bags on students‚Äô health at primary level in district Kahuta. Naturally the study was descriptive and survey method was used, the population consists of one hundred ninety teachers and a sample of one hundred twenty seven teachers was selected using non probability sampling technique. A likert scale questionnaire was developed validated and distributed among the sampled respondents. The researcher personally visited the schools and collected the filled questionnaire. The data was coded and fed to the SPSS to analyze and interpret. The Chi Square test was applied to see the effect of heavy school bags on student‚Äôs health and academic achievement. The study found that heavy bags have negative effect on their health as well as their academic achievement. Students were found complaining their sickness, body and back pain. They were also found improper in their gait and their body postures. The researcher recommended the policy makers to take and develop strategies to decrease the heavy school bags. The school administration needs to make alternate days‚Äô time tables of the subjects. Keyswords: Health, Primary Level, School, Bags, Students Heavy Pages: 456-466 Article: 39 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)39 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)39 Download Pdf: download pdf view article Creative Commons License Exploring the ‚ÄòCivil Repair‚Äô Function of Media: A Case Study of The Christchurch Mosques Shootings Authors: Ayaz Khan Dr. Muhammad Junaid Ghauri Riffat Alam Abstract: This research endeavor is an attempt to explore and analyze the discourse produced by The New Zealand Herald; a newspaper from New Zealand and by The News International; a Pakistani newspaper. The researchers intend to determine whether and to what extent both the newspapers have the role of ‚Äòcivil repair‚Äô played after the Christchurch mosques shootings. The researchers have incorporated the ‚Äòlexicalization‚Äô and the ‚Äòideological square‚Äô techniques proposed by Tuen A. van Dijk within the scope of Critical Discourse Analysis. The findings of this study show that both the selected newspapers assuming the social status of ‚Äòvital center‚Äô performed the role of ‚Äòcivil repair‚Äô in the aftermath of the shootings by producing the ‚Äòsolidarity discourse‚Äô. The ‚Äòsolidarity discourse‚Äô has been produced in terms of the ‚Äòwe-ness‚Äô, harmony, understanding, and by mitigating the conflicting opinions. Keyswords: Christchurch Mosque Shootings, Civil Repair, Civil Sphere Theory, Lexicalization, Solidarity Discourse Pages: 467-484 Article: 40 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)40 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)40 Download Pdf: download pdf view article Creative Commons License China Pakistan Economic Corridor: Regional Dominance into Peace and Economic Development Authors: Tayba Anwar Asia Saif Alvi Abstract: The purpose of this qualitative study was to investigate the true motivations behind CPEC idea and the advantages it delivers to Pakistan and China. It also recognizes the Corridor&apos;s potential for mixing regional economies while dissolving geographical borders. The study is deductive in character, since it examines financial, political, and military elements of Pakistan and China&apos;s positions and situations. Enhancing geographical linkages through improved road, train, and air transport systems with regular and free exchanges of development and individual‚Äôs interaction, boosting through educational, social, and regional civilization and wisdom, activity of larger quantity of investment and commerce flow, generating and moving energy to provide more optimal businesses for the region. Keyswords: Geographical Linkages, Globalized World, Landlocked, Regional Connectivity, Regionalization Pages: 485-497 Article: 41 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)41 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)41 Download Pdf: download pdf view article Creative Commons License China‚Äôs New Great Game in Central Asia: Its Interest and Development Authors: Bushra Fatima Rana Eijaz Ahmad Abstract: Central Asia is rich in hydrocarbon resources. It‚Äôs geostrategic, geopolitical, and geo-economic significance has grasped the attention of multiple actors such as China, the USA, Russia, Turkey, the European Union, Pakistan, Afghanistan, and India. Due to its location, the Central Asian region appeared as a strategic hub. In the present scenario, China‚Äôs strategy is massive economic development, energy interest, peace, and stability. This article highlights China‚Äôs interest, political and economic development, and its role as a major player in the New Great Game in Central Asia. Shanghai Cooperation Organization (SCO) which presents as a platform where China is playing an active role in political, economic, and security concerns for achieving its objectives in Central Asia. The new step of the Belt and Road Initiative (BRI) sheds light on China‚Äôs progressive move in this region via land and sea routes, which creates opportunities for globalization. Keyswords: Belt and Road Initiative, Central Asia, China, New Great Game Pages: 498-509 Article: 42 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)42 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)42 Download Pdf: download pdf view article Creative Commons License Personality Traits as Predictors of Self-Esteem and Death Anxiety among Drug Addicts Authors: Umbreen Khizar Saira Irfan Iram Ramzan Abstract: This study seeks to investigate whether personality traits predict self-esteem and death anxiety among drug addicts. The sample consisted of 100 drug addicts taken from the two hospitals in Multan city. Only men between the ages of 20 and 65 were included in the study. Data was collected through reliable and valid questionnaires. Results revealed positive relationship between conscientiousness, openness to experience and self-esteem. Moreover, findings showed positive relationship between extraversion and death anxiety, and negative correlation between neuroticism and death anxiety. Findings also showed that self-esteem and death anxiety are significantly and negatively correlated. Additionally, findings revealed that conscientiousness positively predicted self-esteem and neuroticism negatively predicted death anxiety. Furthermore, significant differences were observed in self-esteem, and death anxiety based on age. Significant differences were also found in extraversion, agreeableness, openness to experience, and death anxiety based on location. Understanding how personality traits affect behavior can help drug addicts get the support they need to live a better life and reduce their risk of death anxiety and premature death. Keyswords: Death Anxiety, Drug Users, Personality Traits, Self- Esteem Pages: 510-524 Article: 43 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)43 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)43 Download Pdf: download pdf view article Creative Commons License Middle East: A Regional Instability Prototype Provoking Third Party Interventions Authors: Waseem Din Prof. Dr. Iram Khalid Abstract: Third party interventions always prolong the interstate or civil wars with unending sufferings and devastations. The entire Middle East region is fraught with tensions, conflicts, civil wars and rivalries. From strategic interests to power grabbing, sectarian divisions, flaws in the civil and social structure of the state and society, ethnic insurrections, and many other shapes of instability syndromes can be diagnosed in this region. In the post-Arab Spring, 2011, the emerging new regional hierarchical order for power/dominance, in addition to the weakening/declining dominant US power in the region, changed the entire shape of already conflict-ridden region. New weak or collapsing states and bifurcation of the ‚Äòstatus quo‚Äô and ‚Äòcounter-hegemonic‚Äô states along with their respective allies, made this region a prototype of instability in the regional security complex of the Middle East, as a direct result of these developments. The perpetuation of these abnormalities would not recede this instability conundrum from the region, provoking third party intervention, if not contained. Keyswords: Conflicts/Civil Wars, Dominant Power, Instability, Intervention, Middle East, Middle Powers, Regional Hierarchy, Regional Powers, Security Complex, Weak State Pages: 525-542 Article: 44 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)44 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)44 Download Pdf: download pdf view article Creative Commons License Impact of Classroom Environment on Second Language Learning Anxiety Authors: Zohaib Zahid Abstract: Second language learning anxiety has attained the attention of the researchers in almost every part of the world. Pakistan is a country where English is taught as a second language from the very beginning of school education. Second Language learning anxiety is a phenomenon which has been prominently found among the learners because of their less proficiency in learning English language. This study has been conducted to investigate the effect of anxiety in learning and using English language in classroom, university and outside the classroom. There are variables that affect language learning performance of the learners but this paper has solely investigated the effect of anxiety. The paper has concluded that anxiety is a variable which has a striking affect in second language learning and its use inside classrooms. Keyswords: Effect of Anxiety, Proficiency, Second Language Learning Anxiety, Striking Affect Pages: 485-497 Article: 45 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)45 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)45 Download Pdf: download pdf view article Creative Commons License Struggling for Democracy: A Case of Democratization in Pakistan Authors: Ammara Tariq Cheema Dr. Rehana Saeed Hashmi Abstract: The objective of this research paper is to review the challenges for democratization in Pakistan. The problem of democratization and consolidation refers to the structure of democracy following the collapse of non-democratic regime. Ten factors as given by Michael J. Sodaro are considered effective in helping a democratically unstable state to stabilize its system in other words helps in the democratic consolidation. It is argued in this research that the ten factors of democratization as given by Michael J. Sodaro have been absent in the political system of Pakistan and working on these factors can lead Pakistan to the road of democratization. This study uses qualitative method of research and proposes a novel framework for the deed of parliament, because the effectiveness of parliament can contribute positively to democratization/consolidated democracy. Keyswords: Electoral Politics, General Elections, Political Participation, Women Empowerment Pages: 554-562 Article: 46 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)46 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)46 Download Pdf: download pdf view article Creative Commons License Impact of Dependency Ratio on Economic Growth among Most Populated Asian Countries Authors: Dilshad Ahmad Salyha Zulfiqar Ali Shah Abstract: Demographic transition through different channels significantly influences economic growth. Malthusian view postulated as dependency ratio adversely affects economic growth while Julian Simon&apos;s view is quite different, highlighted the long-run benefits of the population in the range of 5 to15 years on economic growth. This study can be a valuable addition in research to analyzing the association of dependency ratio and economic growth of the five most populated Asian countries (Bangladesh, China, Indonesia, India, and Pakistan). Empirical findings of the study indicated that a total dependency and younger dependency ratio has a positive and significant influence on economic growth in both short-run and long-run scenarios while the old dependency ratio shows a negative influence on economic growth in the long run while short-run results are unpredictable. There is a need for state-based proper policy measures in focusing the higher financing in human capital development specifically in education and health. Keyswords: Economic Growth, Gross Saving, Old Dependency Ratio, Young Dependency Ratio Pages: 563-579 Article: 47 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)47 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)47 Download Pdf: download pdf view article Creative Commons License Chinese Geo-Strategic Objectives and Economic Interests in Afghanistan under President Xi Jinping Authors: Farooq Ahmed Prof. Dr. Iram Khalid Abstract: China has its own distinctive interests, concerns and strategies with respect to the changing security dynamics in Afghanistan. China has taken an active interest, though retaining a low profile and avoiding direct military interaction. China has exclusively relished on economic engagement actively and provided numerous financial aid and financial support in the rebuilding of Afghanistan&apos;s economy. The aim of this research study is to analyze the geo-strategic objectives and economic interests of China under the leadership of President Xi Jinping. This study looks at the actual diplomatic, economic and protection commitments of both countries as well as the basis of the geopolitical complexities ‚Äì core variables that form China&apos;s current foreign policy to Afghanistan. Keyswords: Afghanistan, BRI, China, NATO Withdrawal Pages: 580-592 Article: 48 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)48 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)48 Download Pdf: download pdf view article Creative Commons License The Argument Structure of Intransitive Verbs in Pashto Authors: Abdul Hamid Nadeem Haider Bukhari Ghani Rehman Abstract: This study focuses on the description and categorization of intransitive verbs in terms of its argument structure. The study concludes that the unaccusative verbs only project an internal argument. It does not require the event argument. However, the said verb can be causativised by adding external argument and at the same time the event argument gets included in the valency of the derived causative of the unaccusative root. The unergative, on the other hand, requires an external argument as an obligatory argument while the internal argument is not the obligatory argument of the verb. The event argument is also a part of the valency of the verb. The APFs require one argument which is the internal argument of the verb. However, since the external argument is not available, the internal argument of the verb gets realized as the subject of the verb. The verb does not project event argument. The ergative predicates are derived by the suppression of the external argument and by the externalization of the internal argument. Keyswords: Argument Structure, Ergative Case, Event Argument, External Argument, Internal Argument, Valency Pages: 593-610 Article: 49 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)49 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)49 Download Pdf: download pdf view article Creative Commons License Positive, Negative and Criminal Orientation of Beggars in Okara: Perspective of Students Authors: Shahzad Farid Saif-Ur-Rehman Saif Abbasi Hassan Raza Abstract: This study aimed to measure the perspective of students about the criminal orientation of beggars. The sample size of the study (i.e., 100 students) was explored using Taro Yamane‚Äô equation from the university of Okara, Punjab, Pakistan. The respondents were approached using simple random sampling and interviewed using face to face interview schedule. The data was collected using a structured questionnaire. The analysis was administered through SPSS-20.The study explored that parental illiteracy is associated with the high criminal and negative orientation of students towards beggars. It was also explored that females and respondents from rural background have low negative orientation towards beggars. However, males and respondents from urban background have medium criminal orientation and low positive orientation towards beggars, respectively. The study is useful for the government of Punjab, Pakistan campaign and policy for anti-begging. The study introduced the geometrical model of youth‚Äôs orientation toward begging. The study also contributed to the literature on begging by extending its domain from Law and Criminology to sociology as it incorporated social variables e.g., parents‚Äô education, gender, etc., to explore their association with the youth‚Äôs socialization about begging. Keyswords: Begging, Crime, Education, Gender, Students Pages: 611-621 Article: 50 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)50 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)50 Download Pdf: download pdf view article Creative Commons License Relationship between Entrepreneurial Export Orientation and Export Entrepreneurship through Mediation of Entrepreneurial Capabilities Authors: Muhammad Saqib Nawaz Masood ul Hassan Abstract: Export led growth is prominent paradigm in developing world since decades. Exports play vital role in the economy by improving the level of balance of payments, economic growth and employment. Due to strategic importance of exports, organizational researchers focused on finding antecedents of export performance of the organizations. To line with this, current study aims to find the impact of entrepreneurial export orientation on export entrepreneurship through mediation of entrepreneurial capabilities in the Pakistani context. For this purpose, data was collected from 221 exporting firms of Pakistan by using questionnaire. Collected data was analyzed with the help of Smart PLS. In findings, measurement model confirmed the validity and reliability of measures of variables. Additionally, structural model provides the positive impact of entrepreneurial export orientation on export entrepreneurship. Similarly, entrepreneurial capabilities mediate the relationship between entrepreneurial export orientation on export entrepreneurship. The findings provide important implications for the managers of exporting firms to improve export performance. Keyswords: Entrepreneurial Capabilities, Entrepreneurial Export Orientation, Export Entrepreneurship Pages: 622-636 Article: 51 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)51 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)51 Download Pdf: download pdf view article Creative Commons License China Pakistan Economic Corridor: Explaining U.S-India Strategic Concerns Authors: Nasreen Akhtar Dilshad Bano Abstract: Regional and International political and economic landscape is being changed owing to China Pakistan Economic Corridor (CEPEC)-the new security paradigm has taken place-that has increased the strategic concerns of the U.S. and India. This research paper attempts to re-examine China-Pakistan relations in the new emerging geo-political compass. This paper has investigated the question that how regional, and global developments have impacted the China-Pakistan relationship? And why China ‚Äì Pakistan have become partners of CPEC? In the global context, this paper assesses the emerging International Order, Indo-U. S strategic narrative vis-√†-vis CPEC, and the containment of China through the new alliances and their impacts on China -Pakistan vis-√†-vis the Belt Road Initiative (BRI). Quadrilateral (Quad) alliances is shaping the new strategic political and security paradigms in the world politics. Keyswords: BRI, China, CPEC, India, Pakistan, Silk Road, Strategic Concerns Pages: 637-649 Article: 52 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)52 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)52 Download Pdf: download pdf view article Creative Commons License The Structure of Domestic Politics and 1973 Constitution of Pakistan Authors: Dr. Fida Bazai Dr. Ruqia Rehman Amjad Rashid Abstract: Pakistan is located in a pivotal region. Its geo-strategic location affects its national identity as a nation state. Unlike Europe in South Asia security dilemma, proxy warfare and nuclear arms race are consistent features of the regional politics. The identity of Pakistan as security-centric state gives its army disproportional power, which created institutional imbalance that directly affected constitutionalism in the country. The constitution of Pakistan is based on principles of civilian supremacy and separation of power but in reality Pakistan‚Äôs army is the most powerful institution in country. This paper argues that the structure of Pakistani politics; created institutional imbalances by the disproportionate distribution of resources is the key variable in creating dichotomy. The structure of domestic politics is based upon the principles of hostility to India, use of Islam for national unity and strategic alliances with major powers to finance defense against the neighboring countries. Keyswords: Constitutionalism, Identity, Islam, South Asia Pages: 650-661 Article: 53 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)53 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)53 Download Pdf: download pdf view article Creative Commons License National Integration and Regionalism in Pakistan: Government‚Äôs Strategy and Response toward Regionalist Demands 1947-77 Authors: Najeeb ur Rehman Mohammad Dilshad Mohabbat Muhammad Wahid Abstract: The countries of South Asian region have pluralistic societies with different language, religious, and ethnic identities. Pakistan is no exception who is facing the challenge of regionalism since its inception. Different ethnic groups have been consistently raising their voices for separatism or autonomy within the frame work of an existing territorial state. The issues of provincialism, ethnicity, and regionalism is posing a serious challenge to the integrity of the country. This paper aims to explore the causes of the regionalism in Pakistan and intends to analyze the policies and strategies of different political governments which they launched to tackle this all important issue. The paper follows the historical method of research and analyzes different types of qualitative data to conclude the finding of the research. The paper develops the theory of ‚ÄúRegionalists Demand and Government Response‚Äù which shows how different regionalist forces put their demands and how the governments react on these demands. It recommends the grant of greater regional autonomy to the regionalists to enhance internal security and to protect the country from disintegration. Keyswords: Demands, Ethnicity, Government Strategy, National Integrity, Nationalism, Regionalism Pages: 662-678 Article: 54 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)54 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)54 Download Pdf: download pdf view article Creative Commons License Fostering Entrepreneurial Mindset through Entrepreneurial Education: A Qualitative Study Authors: Saira Maqbool Dr. Qaisara Parveen Dr. Muhammad Hanif Abstract: Research on entrepreneurial mindset has flourished in these recent years. Its significance lies in a critical suspicion and its matters for inventive behavior. Entrepreneurship joined with innovative abilities, seen as one of the most wanted in this day and age. This study aims to determine the perceptions about entrepreneurial mindset, its importance, and the role of entrepreneurship education and Training in developing the entrepreneurial mindset. This is a qualitative study based on interviews conducted by professors of Pakistan and Germany. The analysis was determined through content analysis. The results determine that &apos;Making Entrepreneurial Mindset&apos; assists with seeing better all parts of business venture, which will undoubtedly influence their view of business venture, pioneering abilities, and mentalities. Keyswords: Entrepreneurship Education, Entrepreneurial Mindset Pages: 679-691 Article: 55 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)55 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)55 Download Pdf: download pdf view article Creative Commons License Benefits of Implementing Single National Curriculum in Special Schools of Lahore city for Children with Intellectual Disability: Teachers‚Äô Perception Authors: Dr. Hina Fazil Khurram Rameez Sidra Ansar Abstract: Single national curriculum (SNC) is an important issue across the Punjab Province of Pakistan. Making and implementing SNC is not only focusing the education of normal pupils, but also focusing students with disabilities (SWD). The field of special education experienced an increased discussion of curriculum for students with intellectual disabilities (SID). The present research aimed to know the benefits to implement first stage of single national curriculum for students with Intellectual disability and to know the differences about the benefits between public and private schools regarding SNC for students with ID based on demographic characteristics. Likert type researchers-made questionnaire with reliability) Cronbach alpha .922) was used. 90 special educationists from public and private schools were chosen through random sampling technique. The findings raised some benefits such as: SNC will bridge the social and economic disparities which will increase the acceptance of ID students. It was recommended that SNC should include areas of adaptive skills, motor, and vocational skills to get involved in work activities. Keyswords: Benefits, Children with Intellectual Disability, Single National Curriculum Pages: 692-703 Article: 56 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)56 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)56 Download Pdf: download pdf view article Creative Commons License Last Rituals and Problems Faced by the Hindu Community in Punjab: A Case Study of Lahore Authors: Sabir Naz Abstract: Lahore is the provincial capital of Punjab, where a sizeable population of the Hindus has been residing there since the inception of Pakistan. There had been many crematoriums in the city but with the passage of time, one after another, disappeared from the land after partition of the Sub-continent. Those places were replaced by commercial or residential sites. There is also a graveyard in the city which is in the use of Hindu Valmik Sect. However, it was encroached by some Muslims due to very small size of population and indolence of the Hindus. Later on, the encroachments were removed by the District Government Lahore in compliance of order of the Supreme Court of Pakistan. Presently, there is a graveyard as well as a crematorium in the city. The community remained deprived of a place to dispose of a dead body according to their faith for a long period which is contravention with the guidelines of the Quaid-e-Azam, founder of the nation Keyswords: Crematorium, Graveyard, Hindu community, Last Rituals Pages: 704-713 Article: 57 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)57 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)57 Download Pdf: download pdf view article Creative Commons License Estimating Growth Model by Non-Nested Encompassing: A Cross Country Analysis Authors: Benish Rashid Dr. Shahid Razzaque Dr. Atiq ur Rehman Abstract: Whether models are nested or non-nested it is important to be able to compare them and evaluate their comparative results. In this study six growth models have been used for analyzing the main determinants of economic growth in case of cross countries, therefore by using these six models we have tested them for non-nested and nested encompassing through Cox test and F-test respectively. Data from 1980 to 2020 were used to analyze the cross country growth factors so therefore, the current study looked at about forty four countries with modelling these different comparative studies based on growth modelling. So, we can make these six individual models and we can estimate the General Unrestricted Model with the use of econometric technique of Non-Nested Encompassing. By evaluating the data using the Non-Nested Encompassing econometric technique, different sets of economic variables has been used to evaluate which sets of the economic variables are important to boost up the growth level of the country. And found that in case of nested model or full model it is concluded that model with lag value of GDP, trade openness, population, real export, and gross fix capital formation are the main and potential determinants to boost up the Economic Growth in most of the countries. Keyswords: Cross Country, Economic Growth, Encompassing, Nested, Non-nested Pages: 714-727 Article: 58 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)58 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)58 Download Pdf: download pdf view article Creative Commons License Assessment of Youth Buying Behaviour for Organic Food Products in Southern Punjab: Perceptions and Hindrances Authors: Ayousha Rahman Asif Yaseen Muhammad Arif Nawaz Abstract: This research examined the cognitive antecedental effects on organic food purchase behaviour for understanding the perceptions and hindrances associated with purchasing organic food products. Theory of Planned Behaviour (TPB) was adopted as a theoretical framework. A total of 250 young consumers in the two cities of Southern Punjab, Pakistan was randomly sampled and data were collected via a face-to-face survey method. Partial least square technique was employed to test the model. The results showed that attitude towards organic food purchasing motivated when moral norms were activated to consume organic food products. Further, environmental knowledge moderated the relationship of organic food purchase intentions and behaviour significantly. The findings highlighted the importance of moral norms as a meaningful antecedent that could increase the TP-based psychosocial processes if consumers have sufficient environmental knowledge. Therefore, farmers, organic products marketers, government administrators, and food retailers should take initiatives not only to highlight the norms and values but also when promoting organic food production and consumption. Keyswords: Environmental Knowledge, Organic Food Purchase Behaviour, Personal Attitude, PLS-SEM, Subjective &amp; Moral Norms Pages: 728-748 Article: 59 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)59 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)59 Download Pdf: download pdf view article Creative Commons License An Analysis on Students Ideas about English and Urdu as Medium of Instructions in the Subjects of Social Sciences studying in the Colleges of the Punjab, Pakistan Authors: Ashiq Hussain Asma Amanat Abstract: The worth and usefulness of English education as a foreign language is of great concern to language rule and planning (LRP) researchers compared to teaching their native language globally in higher education. The study under research examines the perspectives of two similar groups of the final year students of at Higher Education Institutions of Pakistan. The first group consists of art students who received the Urdu medium of instruction (UMI), and the second group received the English medium of instruction (EMI). An empirical methodology was carried out in the present year, students answered questionnaires to find out the benefits and challenges of learning subject-based knowledge, what subject-based knowledge means to them, and their understanding of language as a teaching language. Interviews were conducted with the selected group of students who wished to participate in research. Additional information is available from the tests and results obtained in the two equivalent courses. Although many similarities have been identified between the two groups, the overall knowledge of disciplinary knowledge of English medium instruction students was not very effective, while that of UMI students was very effective. It explains the implications of the findings to continue the language rule as policy experience for teaching in higher education institutions. Keyswords: English as Medium of Instruction (EMI), Higher Education Institutions (HEIs), Urdu as Medium of Instruction (UMI) Pages: 749-760 Article: 60 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)60 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)60 Download Pdf: download pdf view article Creative Commons License Environment and Women in Kurt Vonnegut‚Äôs ‚ÄòHappy Birthday Wanda Juny‚Äô: An Eco- Critical and Feminist Analysis Authors: Dr. Muhammad Asif Safana Hashmat Khan Muhammad Afzal Khan Janjua Abstract: This is an Eco-feminist study of Vonnegut‚Äôs ‚ÄòHappy Birthday Wanda Juny‚Äô and focuses on how both women and environment are exploited by patriarchy. Ecofeminism critiques masculine dominance highlighting its role in creating and perpetuating gender discrimination, social inequity and environmental degradation. Women suffer more because of power disparity in society. Environmental crises affect women more than men because of their already precarious existence and subaltern position. There is affinity between women and nature are victims of climate change and other environmental hazards. Cheryl Glotfelty introduced interdisciplinary approach to the study of literature and environment. Literary ecology as an emerging discipline explores the intriguing relationship between environment and literature. Ecofeminism draws on feminist critique of gender inequality showing how gender categories inscribed in power structure exploit both women and nature. Francoise d‚ÄòEaubonne coined the term ecofeminism to critique the prevalent exploitation of both women and environment. Ecofeminism asserts that exploitation of women and degradation of the environment are the direct result of male dominance and capitalism. Ecofeminism argues for redressing the plight of women and protection of environment. Vonnegut‚Äôs play ‚ÄòHappy Birthday Wanda June‚Äô was written at a time when the movement for the right of women and protection of environment were gaining momentum. The play shows how toxic masculinity rooted in power and capitalism exploit both women and environment. Keyswords: Eco-Feminism, Eco-Criticism, Ecology, Environment, Exploitation Pages: 761-773 Article: 61 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)61 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)61 Download Pdf: download pdf view article Creative Commons License Critical Analysis of Social Equity and Economic Opportunities in the Light of Quranic Message Authors: Prof. Dr. Muhammad Yousuf Sharjeel Mahnaz Aslam Zahida Shah Abstract: This study critically evaluated the key verses of Surah Al-Baqarah -the second chapter of Quran, a sacred scripture of Islam- which specifically relates to social equity opportunities and a code of conduct in the context of economics. The Quran claims that it is a book which explains every situation; therefore, the aim of this study remained to extract those verses of Surah Al-Baqarah which can guide us in Economics. The authentic and approved Islamic clerics and their translations were consulted for the interpretations of the Holy verses. The researchers chiefly focused and studied Surah Baqarah with regards to social equity and economic opportunities. The translations were primarily in the regional language Urdu so the interpretations must not be related exactly equitable in English. The study engaged the document analysis research strategy. This study is only an endeavour to decipher Holy Quran‚Äôs message from Allah for the mankind so it must not be considered as the full and complete solution to the all the economic issues, challenges and opportunities. Ahadees and the saying of the Holy prophet were referred to where ever required and available. The researcher also considered the Tafasir (detail intellectual interpretations) of the Quran done by the well-known scholars of Islam for the verses studied therein and any statements and/or material - such as ideas, studies, articles, documentation, data, reports, facts, statistics etc. For the study, data was collected and analyzed qualitatively. On the basis of the study, recommendations were also primed. Keyswords: Economic Issues and Challenges, Social Equity, Surah Al-Baqarah, Al Quran Pages: 774-790 Article: 62 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)62 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)62 Download Pdf: download pdf view article Creative Commons License A Critical Discourse Analysis of Dastak by Mirza Adeeb Authors: Muhammad Afzal Dr. Syed Kazim Shah Umar Hayat Abstract: The present research aims to explore ideology in Pakistani drama. The drama, ‚ÄúDastak‚Äù, written by Mirza Adeeb, has been taken for exploration ideologically. Fairclough‚Äôs (1992) three-dimensional model has been used for analyzing the text of the above-mentioned drama which includes textual, discursive practice and social practice analyses. The linguistic and social analyses of the drama reveal the writer‚Äôs ideology about socio-cultural, conventional and professional aspects of life. The study has also explored the past and present states of mind of Dr. Zaidi, the central and principal character of the drama, Dastak. The text implies that the writer has conveyed personal as well as social aspects of his times through the drama of Dastak. Keyswords: Dastak, Drama, Ideology, Semiotics Pages: 791-807 Article: 63 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)63 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)63 Download Pdf: download pdf view article Creative Commons License Linking Job Satisfaction to Employee Performance: The Moderating Role of Islamic Work Ethics Authors: Dr. Shakira Huma Siddiqui Dr. Hira Salah ud din Khan Dr. Nabeel Younus Ansari Abstract: The most pervasive concern in public sector organizations is declining employee performance and workforce of these organizations are less satisfied with their jobs. The aim of this study is to investigate the impact of Job Satisfaction on employee‚Äôs performance and how Islamic work ethics moderates the above mentioned direct relationship in the public sector organizations of Pakistan. The data were collected from the sample of 193 permanent employees working in public sector organizations through stratified sampling technique. The results revealed that employees Job satisfaction is significantly related to higher performance. Further, the findings indicated that Islamic work ethics moderates the relationship between job satisfaction and employee performance. The present research has some theoretical and empirical implications for academicians, policymakers, especially of public sector organizations, for the improvement of performance of their workforce. Keyswords: Employee Performance, Islamic Work Ethics, Job Satisfaction, Person-Environment Fit Theory Pages: 808-821 Article: 64 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)64 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)64 Download Pdf: download pdf view article Creative Commons License Semantics of Qawwali: Poetry, Perception, and Cultural Consumption Authors: Rao Nadeem Alam Tayyaba Khalid Abstract: Semantics is about meanings and meanings are arbitrary and shared. Understanding qawwali context requires comprehension of semantics or process of meaning creation and meaning sharing among the qawwal party and the audience. This interactive activity might frequently be hindered when interrupted by subjective meanings creation during cultural consumption. Qawwali is a cultural tradition, its semantics are conditioned by axiological premises of poetry and perceptions which are transforming. The previous researches revealed that qawwali is associated with religion which provides the religious message by singing hamd and naat. It was a means to experience Divine; therefore, semantics are multi-layered and often crossroad with values and subjective experiences. It is novel due to its ritual of Sama. It has the therapeutic power that helps mentally disturbed people and they find refuge. This study is exploratory having a small sample size of twenty purposively selected audiences. This phenomenological inquiry used ethnographic method of conversational interviews at selected shrines and cultural spaces in Islamabad. The results indicate that qawwali is a strong refuge for people facing miseries of life and they attend Sama with a belief that attending and listening will consequently resolve their issues, either psychological or physiological. They participate in Sama which teaches them how to be optimistic in a negative situation; this paper brings forth this nodal phenomenon using the verbatim explanations by the interlocutors. Semantics of Qawwali are conditioned and some of these elements are highlighted including poetry and axiology based perceptions and cultural consumption of a cultural realm. Keyswords: Cognition, Culture, Poetry, Qawwal, Qawwali, Semantics Pages: 822-834 Article: 65 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)65 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)65 Download Pdf: download pdf view article Creative Commons License Political Economy of Smuggling: The Living Source for the Natives (A Case Study of Jiwani-Iran Border, Baluchistan) Authors: Abdul Raheem Dr. Ikram Badshah Wasia Arshed Abstract: This study explores the political economy of smuggling on Jiwani-Iran border. The natives are majorly involved in illegal transportation of goods and objects, therefore; the study sets to explain how significant smuggling for the local people is. It describes the kinship role in reciprocity of their trade and transportation. The qualitative methods such as purposive sampling and interview guide were employed for data collection. The research findings revealed that local people were satisfied with their illegal trading which is depended largely on their expertise and know-how of smuggling at borders. They disclosed that their total economy was predominantly based on smuggling of stuff like drugs, diesel, oil, gas, petrol, ration food from Iran, and human trafficking. They also enjoyed the privilege of possessing Sajjil (Iranian identity card), thus; the dual nationality helped them in their daily business and rahdari (border crossing agreement), enabling them to travel to Iran for multiple purposes. Keyswords: Drugs, Human, Navigation, Political Economy, Reciprocity, Smuggling, Trafficking Pages: 835-848 Article: 66 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)66 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)66 Download Pdf: download pdf view article Creative Commons License The Vicious Circles of System: A Kafkaesque Study of Kobo Abe‚Äôs The Woman in the Dunes Authors: Imran Aslam Kainat Azhar Abstract: This paper analyses the Kafkaesque/Kafkan features of Kobo Abe‚Äôs novel The Woman in the as formulated by Kundera in ‚ÄúKafka‚Äôs World.‚Äù For Kundera, in a Kafkaesque work human existence is bleakly represented through intermingling of tragedy and comedy in an indifferent world dominated by hegemonic systems. The Kafkaesque is characterised by the following: World is a huge forking labyrinthine institution where the man has been thrown to suffer its complexities, confrontation with the labyrinth makes his existence meaningless because freedom is a taboo in no man‚Äôs land, he is punished for an unknown sin for which he seeks justification from the superior authorities, but his efforts are viewed as ludicrous or comic despite the underlying sense of tragedy. (5) The Kafkaesque tendency to present tragic situation comically is also explored in Abe‚Äôs novel. The paper studies the effect of higher authorities exercising their power over man and the inscrutability of cosmic structures continuously undermining human freedom in nightmarish conditions. The paper establishes Kobo Abe in the literary world as a writer who portrays the hollowness and futility of human lives with a Kafkaesque touch. Keyswords: Authority, Institutions, Kafka, Kafkaesque, Kafkan, Kobo Abe, Kundera, The Trial, The Woman in the Dune Pages: 849-861 Article: 67 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)67 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)67 Download Pdf: download pdf view article Creative Commons License Subjectivity and Ideological Interpellation: An Investigation of Omar Shahid Hamid‚Äôs The Spinner‚Äôs Tale Authors: Hina Iqbal Dr. Muhammad Asif Asia Saeed Abstract: Louis Althusser‚Äôs concept of interpellation is a process in which individuals internalize cultural values and ideology and becomes subject. Althusser believes that ideology is a belief system of a society in which ideological agencies establish hierarchies in society through reinforcement and discrimination for cultural conditioning. These agencies function through ideological state apparatuses. These ideological agencies help to construct individual identity in society. The undesirable ideologies promote repressive political agendas. The non-repressive ideologies are inhaled by the individuals as a natural way of looking at the culture and society. This research seeks to investigate Omar Shahid Hamid‚Äôs novel The Spinners Tales through the lens of Althusser‚Äôs ideology and interpellation. This study examines how the characters of Shahid‚Äôs novel inhaled ideology and became its subjects. This research also depicts the alarming effects of cultural hegemony that creates cultural infidelity and hierarchies between the bourgeoisie and proletariat classes. Keyswords: Cultural Hegemony, Ideological State Apparatus, Ideology, Interpellation, Repressive Factors Pages: 862-872 Article: 68 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)68 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)68 Download Pdf: download pdf view article Creative Commons License Blessing in Disguise: Recommendations of Indian Education Commission (1882) and Christian Missionaries‚Äô Educational Policy in the Colonial Punjab Authors: Mohammad Dilshad Mohabbat Muhammad Hassan Muhammad Ayaz Rafi Abstract: Woods Education Despatch is considered to be the Magna Carta of Indian Education. It controlled the Indian education field till the establishment of Indian Education Commission, 1882. The Despatch provided space to Christian missionaries by promising government‚Äôs gradual withdrawal from the education in favour of missionaries. It also facilitated the missionaries by offering system of ‚Äògrants on aid‚Äô to the private bodies. Consequently, the missionaries fancied to replace the government institutions in the Punjab and initiated their efforts to increase the number of their educational institutions. They tried to occupy the educational field by establishing more and more educational institutions. But after the Recommendations of the Indian Education Commission 1882, a change in their policy of numeric increase of educational institutions is quite visible. With the turn of the century, they are found to be eager to establish a few institutions with good quality of education. This paper intends to analyse different factors behind the change of their policy of quantitative dominance to qualitative improvement. It also attempts to evaluate how their change of policy worked and what steps were taken to improve the quality of their educational institutions. Following the historical method qualitative data comprising educational reports, missionaries‚Äô autobiographies, Reports of missionaries‚Äô conferences, and the other relevant primary and secondary sources has been collected from different repositories. The analysis of the data suggests that the attitude of the administration of the education department and the recommendations of Indian Education Commission were the major driving forces behind the change of missionaries‚Äô educational policy in the 20th century. The missionaries, after adopting the new policy, worked on the quality of education in their institutions and became successful. Keyswords: Christian Missionaries, Indian Education Commission, Missionary Schools, Numeric Increase, Quality of Education. The Punjab, Woods Education Despatch Pages: 873-887 Article: 69 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)69 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)69 Download Pdf: download pdf view article Creative Commons License Basic Life Values of Prospective Special Education Teachers Authors: Dr. Maria Sohaib Qureshi Dr. Syeda Samina Tahira Dr. Muhammad Irfan Arif Abstract: Future teachers&apos; preconceived values about how to live their lives and how that affects the lives of their students were the focus of this study. Descriptive research was used by the researchers. The study was carried out by using Morris&apos;s Ways to Live Scale. Researchers used this scale to study prospective special education teachers&apos; gender, social status, personal relationships, aesthetics and mental approach using purposive sampling method. Descriptive and inferential stats were used to analyse the data collected from those who participated in the study on basic life values of prospective teachers. Results indicated that being social and sympathetic are the most important values among prospective special education teachers. It was also found that male and female prospective special education teachers living in urban and rural areas had no significant differences in their basic life values. Keyswords: Special Education, Teacher, Values Pages: 888-896 Article: 70 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)70 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)70 Download Pdf: download pdf view article Creative Commons License Perception of Dowry: Effects on Women Rights in Punjab Authors: Dr. Bushra Yasmeen Dr. Muhammad Ramzan Dr. Asma Seemi Malik Abstract: Dowry is a common tradition in south Asian countries, especially in Pakistan and India. Daughters became curses and liability for parents causing serious consequences. For control, there are legal ban/restrictions (Dowry and Wedding Gifts (Restriction) Act, 1976; Amendment in Act, 1993) on its practice in Pakistan. Despite the legal cover, the custom has been extended. Dowry amount seems to be increasing due to changing lifestyle and trends of society. To understand males‚Äô and females‚Äô perceptions about dowry; impacts of dowry; why dowry is essential; and how it is affecting women‚Äôs rights and eventually affecting women‚Äôs autonomy. A qualitative study was conducted. Data was collected by using unstructured interviews from males and females including social activists, economists, and married couples about wedding expenses, demands, society pressure, men‚Äôs support, and perception against dowry especially with regards to women‚Äôs rights and autonomy. The study concluded heavy dowry especially in terms of furniture, electronics, kitchenware, car, furnished houses, and cash highly associated with women‚Äôs development and their rights. General people‚Äôs perception showed that dowry is no longer remained a custom or tradition in Asian countries. It is just a trend and people follow it as a symbol of respect for parents and women as well. Keyswords: Dowry, Effects, Impacts Of Dowry, Perceptions, Women Autonomy, Women Rights Pages: 897-909 Article: 71 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)71 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)71 Download Pdf: download pdf view article Creative Commons License NCOC-An Emblem of Effective Governance: An analysis of Pakistan‚Äôs Counter Strategy for Covid-19 as a Non-Traditional Security Challenge Authors: Dr. Iram Khalid Abstract: COVID -19 affected the world unprecedentedly. Lack of capacity and poor standards of governance caused nontraditional security challenges to Pakistan too. The NCOC is the central nerve center to guide the national response to COVID-19 by Pakistan and can be best analyzed in the light of the decision-making theory of Naturalist Decision Making (NDM). The study points out the effective role performed by NCOC at policy formation through a more prosaic combination of science, data, decision making and execution of decisions at the level of federalism. The study highlights the changing patterns of government‚Äôs approach during the pandemic at various levels. Pakistan faced economic, political and social crisis during this phase. This study uses a survey and key informant interviews as the source of analysis for qualitative data collection. By applying the decision- making theory, the paper extends that there is a need to use a model to balance the existing gap within the system, to meet challenges. The study suggests a coordinating approach among various units and center; that might raise the level of performance to meet the nontraditional security challenges with innovation, creativity and boldness. Keyswords: COVID-19, Decision Making Theory, Governance, Nontraditional Threats, Strategy Pages: 910-930 Article: 72 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)72 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)72 Download Pdf: download pdf view article Creative Commons License Comparative Implications of Wednesbury Principle in England and Pakistan Authors: Safarat Ahmad Ali Shah Dr. Sara Qayum Arzoo Farhad Abstract: Wednesbury principle is one of the most important and useful grounds of the Judicial Review. Judicial review is a remedy provided by the public law and is exercised by the superior and higher courts to supervise administrative authorities&apos; powers and functions. The main objective of the judicial review is to ensure the fair and transparent treatment of individuals by public authorities. The ground of the judicial review, i.e., Unreasonableness or irrationality or popularly known as Wednesbury Unreasonableness was introduced by lord Greene in the Wednesbury Corporation case in 1948. Initially, the scope of this ground of judicial review was very narrow and was allowed only in rare cases. However, with the development of administrative law and Human rights, it also developed. Its development resulted in different controversies and issues about the application of this ground. The main issue is about its encroachment in the jurisdiction of other branches of the government i.e., the parliament and executive. The free and loose application of this principle results in confusion and conflict between different organs of the government. The present paper is based on the implications of the limitations on the ground of Wednesbury Unreasonableness both on the judicial and administrative bodies in Pakistan to avoid the chaos and confusion that results in the criticisms on this ground of judicial review. Keyswords: Administrative Authorities, Critical Analysis, Illegality, Judicial Review, Pakistan, Wednesbury Unreasonableness Pages: 931-946 Article: 73 , Volume 2 , Issue 4 DOI Number: 10.47205/jdss.2021(2-IV)73 DOI Link: http://doi.org/10.47205/jdss.2021(2-IV)73 Download Pdf: download pdf view article Creative Commons License Water Sharing Issues in Pakistan: Impacts on Inter-Provincial Relations</title>
		<author>
			<persName coords=""><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florent</forename><surname>Altch√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno type="DOI">10.47205/jdss.2021(2-iv)74</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Development and Social Sciences</title>
		<title level="j" type="abbrev">JDSS</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin</editor>
		<idno type="ISSN">2709-6254</idno>
		<idno type="ISSNe">2709-6262</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">IV</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
			<date type="published" when="2020">2020</date>
			<publisher>Pakistan Social Sciences Research Institute (PSSRI)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,296.65,216.32,7.77;10,70.03,307.61,216.33,7.77;10,70.03,318.41,151.33,7.93" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pheng</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.14007</idno>
		<idno>2023. 2</idno>
		<title level="m">Joint-mae: 2d-3d joint masked autoencoders for 3d point cloud pre-training</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.04,329.86,216.32,7.77;10,70.03,340.82,216.33,7.77;10,70.03,351.62,216.33,7.93;10,70.03,362.58,138.13,7.93" xml:id="b27">
	<monogr>
		<title level="m" type="main">Calip: Zero-shot enhancement of clip with parameter-free attention</title>
		<author>
			<persName coords=""><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Longtian</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xianzheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xupeng</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14169</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.04,374.04,216.32,7.77;10,70.03,384.83,216.33,7.93;10,70.03,395.79,120.28,7.93" xml:id="b28">
	<analytic>
		<title level="a" type="main">Low-Shot Visual Recognition by Shrinking and Hallucinating Features</title>
		<author>
			<persName coords=""><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.328</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,407.25,216.32,7.77;10,70.03,418.21,216.33,7.77;10,70.03,429.01,213.83,7.93" xml:id="b29">
	<analytic>
		<title level="a" type="main">Masked Autoencoders Are Scalable Vision Learners</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.01553</idno>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<idno>2021. 13</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.04,440.46,216.32,7.77;10,70.03,451.42,216.33,7.77;10,70.03,462.22,216.33,7.93;10,70.03,473.18,216.33,7.93;10,70.03,484.30,109.33,7.77" xml:id="b30">
	<analytic>
		<title level="a" type="main">Masked Autoencoders Are Scalable Vision Learners</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.01553</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,495.60,216.32,7.77;10,70.03,506.55,216.33,7.77;10,70.03,517.35,216.33,7.93;10,70.03,528.31,216.33,7.73;10,70.03,539.43,55.53,7.77" xml:id="b31">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00975</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">June 2020</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,550.73,216.32,7.77;10,70.03,561.53,216.33,7.93;10,70.03,572.48,216.33,7.93;10,70.03,583.60,27.89,7.77" xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking ImageNet Pre-Training</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00502</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">October 2019</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,594.90,216.32,7.77;10,70.03,605.70,216.33,7.93;10,70.03,616.66,216.33,7.73;10,70.03,627.62,133.88,7.93" xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,639.07,216.32,7.77;10,70.03,650.03,216.33,7.77;10,70.03,660.83,216.33,7.93;10,70.03,671.79,216.33,7.73;10,70.03,682.75,185.28,7.93" xml:id="b34">
	<analytic>
		<title level="a" type="main">EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification</title>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Helber</surname></persName>
			<idno type="ORCID">0000-0001-8454-4301</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Bischke</surname></persName>
			<idno type="ORCID">0000-0002-6473-3348</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<idno type="DOI">10.1109/jstars.2019.2918242</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<title level="j" type="abbrev">IEEE J. Sel. Top. Appl. Earth Observations Remote Sensing</title>
		<idno type="ISSN">1939-1404</idno>
		<idno type="ISSNe">2151-1535</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2217" to="2226" />
			<date type="published" when="2019-07">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,694.20,216.32,7.77;10,70.03,705.16,216.33,7.77;10,328.78,75.96,216.33,7.73;10,328.78,86.92,213.37,7.94" xml:id="b35">
	<analytic>
		<title level="a" type="main">Natural Adversarial Examples</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr46437.2021.01501</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06">2021</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,328.79,99.32,216.32,7.77;10,328.78,110.28,216.33,7.77;10,328.78,121.24,216.33,7.77;10,328.78,132.04,216.33,7.93;10,328.78,143.00,161.64,7.93" xml:id="b36">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,328.79,155.39,216.32,7.77;10,328.78,166.19,216.33,7.93;10,328.78,177.15,106.83,7.93" xml:id="b37">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">Tony</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:2204.03649</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2022">2022</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,328.79,189.55,216.32,7.77;10,328.78,200.35,216.33,7.93;10,328.78,211.31,216.33,7.73;10,328.78,222.26,124.92,7.93" xml:id="b38">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName coords=""><forename type="first">Muhammad</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamal</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,328.79,234.66,216.32,7.77;10,328.78,245.62,216.33,7.77;10,328.78,256.58,216.33,7.77;10,328.78,267.38,216.33,7.93;10,328.78,278.34,216.33,7.93;10,328.78,289.46,27.89,7.77" xml:id="b39">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName coords=""><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,328.79,301.69,216.32,7.77;10,328.78,312.49,216.33,7.93;10,328.78,323.45,216.33,7.73;10,328.78,334.57,99.11,7.77" xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey</title>
		<author>
			<persName coords=""><forename type="first">Longlong</forename><surname>Jing</surname></persName>
			<idno type="ORCID">0000-0001-7115-2341</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Yingli</forename><surname>Tian</surname></persName>
			<idno type="ORCID">0000-0003-4458-360X</idno>
		</author>
		<idno type="DOI">10.1109/tpami.2020.2992393</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">1939-3539</idno>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4037" to="4058" />
			<date type="published" when="2021-11-01">2021</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,328.79,346.81,216.32,7.77;10,328.78,357.77,216.33,7.77;10,328.78,368.56,216.33,7.73;10,328.78,379.52,174.08,7.93" xml:id="b41">
	<analytic>
		<title level="a" type="main">3D Object Representations for Fine-Grained Categorization</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2013.77</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision Workshops</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-12">2013</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,328.79,391.92,216.32,7.77;10,328.78,402.88,216.33,7.77;10,328.78,413.84,216.33,7.77;10,328.78,424.64,216.33,7.93;10,328.78,435.60,191.32,7.93" xml:id="b42">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<title level="j" type="abbrev">Commun. ACM</title>
		<editor>F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger</editor>
		<idno type="ISSN">0001-0782</idno>
		<idno type="ISSNe">1557-7317</idno>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2012">2012</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,328.79,447.99,216.32,7.77;10,328.78,458.95,216.33,7.77;10,328.78,469.75,216.33,7.93;10,328.78,480.71,216.33,7.73;10,328.78,491.67,143.10,7.93" xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection</title>
		<author>
			<persName coords=""><forename type="first">Bohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr46437.2021.00728</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06">2021</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,328.79,504.07,216.32,7.77;10,328.78,514.87,216.33,7.93;10,328.78,525.82,216.33,7.73;10,328.78,536.78,98.45,7.93" xml:id="b44">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName coords=""><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.324</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">Oct 2017</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,328.79,549.18,216.32,7.77;10,328.78,560.14,216.33,7.77;10,328.78,571.10,216.33,7.77;10,328.78,581.90,216.33,7.93;10,328.78,593.02,62.89,7.77" xml:id="b45">
	<analytic>
		<title level="a" type="main">Frozen CLIP Models are Efficient Video Learners</title>
		<author>
			<persName coords=""><forename type="first">Ziyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19833-5_23</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
				<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="388" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,328.79,605.25,216.32,7.77;10,328.78,616.21,216.33,7.77;10,328.78,627.17,216.33,7.77;10,328.78,637.97,216.33,7.93;10,328.78,648.93,216.33,7.93;10,328.78,660.05,59.01,7.77" xml:id="b46">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<author>
			<persName coords=""><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.00986</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10">October 2021</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,328.79,672.29,216.32,7.77;10,328.78,683.24,216.33,7.77;10,328.78,694.04,216.33,7.93;10,328.78,705.16,4.48,7.77" xml:id="b47">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<idno>2013. 5</idno>
		<title level="m">Fine-grained visual classification of aircraft</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,70.04,76.13,216.32,7.77;11,70.03,86.92,216.33,7.94;11,70.03,97.88,97.87,7.94" xml:id="b48">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">Sachit</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:2210.07183</idno>
		<idno>2022. 2</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,70.04,110.40,216.32,7.77;11,70.03,121.36,216.33,7.77;11,70.03,132.31,216.33,7.77;11,70.03,143.11,216.33,7.93;11,70.03,154.07,216.33,7.73;11,70.03,165.03,89.89,7.93" xml:id="b49">
	<analytic>
		<title level="a" type="main">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</title>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00272</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">October 2019</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,177.54,216.32,7.77;11,70.03,188.50,216.33,7.77;11,70.03,199.30,216.33,7.93;11,70.03,210.26,216.33,7.73;11,70.03,221.22,186.70,7.93" xml:id="b50">
	<analytic>
		<title level="a" type="main">SLIP: Self-supervision Meets Language-Image Pre-training</title>
		<author>
			<persName coords=""><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19809-0_30</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<title level="s">Proceedings, Part XXVI</title>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="page" from="529" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,233.73,216.32,7.77;11,70.03,244.53,216.33,7.93;11,70.03,255.49,216.33,7.73;11,70.03,266.45,178.71,7.93" xml:id="b51">
	<analytic>
		<title level="a" type="main">Automated Flower Classification over a Large Number of Classes</title>
		<author>
			<persName coords=""><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/icvgip.2008.47</idno>
	</analytic>
	<monogr>
		<title level="m">2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008-12">2008. 2008</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,278.96,216.32,7.77;11,70.03,289.92,216.33,7.77;11,70.03,300.88,216.33,7.77;11,70.03,311.68,216.33,7.93;11,70.03,322.80,129.50,7.77" xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles</title>
		<author>
			<persName coords=""><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_5</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ‚Äì ECCV 2016</title>
				<editor>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,335.15,216.32,7.77;11,70.03,345.95,216.33,7.93;11,70.03,356.91,216.33,7.93;11,70.03,368.03,83.17,7.77" xml:id="b53">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Omkar M Parkhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,380.38,216.32,7.77;11,70.03,391.34,216.33,7.77;11,70.03,402.14,216.33,7.93;11,70.03,413.10,216.33,7.73;11,70.03,424.22,46.56,7.77" xml:id="b54">
	<analytic>
		<title level="a" type="main">Context Encoders: Feature Learning by Inpainting</title>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.278</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,436.57,216.32,7.77;11,70.03,447.53,216.33,7.77;11,70.03,458.33,216.33,7.93;11,70.03,469.45,27.89,7.77" xml:id="b55">
	<monogr>
		<title level="m" type="main">What does a platypus look like? generating customized prompts for zeroshot image classification</title>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.03320</idno>
		<idno>2022. 2</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,70.04,481.80,216.32,7.77;11,70.03,492.76,216.33,7.77;11,70.03,503.72,216.33,7.77;11,70.03,514.68,216.33,7.77;11,70.03,525.48,216.33,7.93;11,70.03,536.60,182.56,7.77" xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,548.95,216.32,7.77;11,70.03,559.91,216.33,7.77;11,70.03,570.87,216.33,7.77;11,70.03,581.66,216.33,7.93;11,70.03,592.62,216.33,7.93;11,70.03,603.58,216.33,7.93;11,70.03,614.70,125.53,7.77" xml:id="b57">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="11,70.04,627.06,216.32,7.77;11,70.03,638.01,211.82,7.77" xml:id="b58">
	<analytic>
		<title level="a" type="main">How smooth are particle trajectories in a ŒõCDM Universe?</title>
		<author>
			<persName coords=""><forename type="first">Cornelius</forename><surname>Rampf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Villone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Uriel</forename><surname>Frisch</surname></persName>
		</author>
		<idno type="DOI">10.1093/mnras/stv1365</idno>
	</analytic>
	<monogr>
		<title level="j">Monthly Notices of the Royal Astronomical Society</title>
		<title level="j" type="abbrev">Mon. Not. R. Astron. Soc.</title>
		<idno type="ISSN">0035-8711</idno>
		<idno type="ISSNe">1365-2966</idno>
		<imprint>
			<biblScope unit="volume">452</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1421" to="1436" />
			<date type="published" when="2015-07-14">2015</date>
			<publisher>Oxford University Press (OUP)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.04,650.37,216.32,7.77;11,70.03,661.33,216.33,7.77;11,70.03,672.29,216.33,7.77;11,70.03,683.08,216.33,7.93;11,70.03,694.04,216.33,7.93;11,70.03,705.16,81.68,7.77" xml:id="b59">
	<analytic>
		<title level="a" type="main">DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</title>
		<author>
			<persName coords=""><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.01755</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">2022</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,76.12,216.32,7.77;11,328.78,87.08,216.33,7.77;11,328.78,97.88,216.33,7.94;11,328.78,109.00,124.27,7.77" xml:id="b60">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,120.33,216.32,7.77;11,328.78,131.28,216.33,7.77;11,328.78,142.08,216.33,7.93;11,328.78,153.04,216.33,7.73;11,328.78,164.00,216.33,7.93;11,328.78,175.12,216.33,7.77;11,328.78,186.08,118.05,7.77" xml:id="b61">
	<analytic>
		<title level="a" type="main">Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p18-1238</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
	<note>: Long Papers)</note>
</biblStruct>

<biblStruct coords="11,328.79,197.40,216.32,7.77;11,328.78,208.20,216.33,7.93;11,328.78,219.16,139.06,7.93" xml:id="b62">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName coords=""><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,230.64,216.32,7.77;11,328.78,241.60,216.33,7.77;11,328.78,252.40,190.68,7.93" xml:id="b63">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName coords=""><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<idno>2012. 5</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,328.79,263.89,216.32,7.77;11,328.78,274.85,216.33,7.77;11,328.78,285.64,216.33,7.93;11,328.78,296.60,216.33,7.73;11,328.78,307.56,153.34,7.93" xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning to Compare: Relation Network for Few-Shot Learning</title>
		<author>
			<persName coords=""><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00131</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,319.05,216.32,7.77;11,328.78,330.00,216.33,7.77;11,328.78,340.80,182.71,7.93" xml:id="b65">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Vishaal</forename><surname>Udandarao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.16198</idno>
		<idno>2022. 3</idno>
		<title level="m">Sus-x: Training-free name-only transfer of vision-language models</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,328.79,352.29,216.32,7.77;11,328.78,363.25,216.33,7.77;11,328.78,374.21,4.48,7.77" xml:id="b66">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,385.53,216.32,7.77;11,328.78,396.49,216.33,7.77;11,328.78,407.45,216.33,7.77;11,328.78,418.41,216.33,7.77;11,328.78,429.20,216.33,7.93;11,328.78,440.16,216.33,7.93;11,328.78,451.28,46.06,7.77" xml:id="b67">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">≈Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,462.61,216.32,7.77;11,328.78,473.57,216.33,7.77;11,328.78,484.36,216.33,7.93;11,328.78,495.32,216.33,7.93;11,328.78,506.44,216.33,7.77;11,328.78,517.40,119.70,7.77" xml:id="b68">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName coords=""><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390156.1390294</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning - ICML &apos;08</title>
				<meeting>the 25th international conference on Machine learning - ICML &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,528.72,216.32,7.77;11,328.78,539.68,216.33,7.77;11,328.78,550.64,216.33,7.77;11,328.78,561.44,216.33,7.93;11,328.78,572.40,216.33,7.93;11,328.78,583.52,55.03,7.77" xml:id="b69">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,594.84,216.32,7.77;11,328.78,605.80,216.33,7.77;11,328.78,616.60,123.52,7.93" xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning to Learn: Model Regression Networks for Easy Small Sample Learning</title>
		<author>
			<persName coords=""><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_37</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ‚Äì ECCV 2016</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="616" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,628.08,216.32,7.77;11,328.78,639.04,216.33,7.77;11,328.78,649.84,216.33,7.93;11,328.78,660.80,216.33,7.73;11,328.78,671.92,119.28,7.77" xml:id="b71">
	<analytic>
		<title level="a" type="main">SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName coords=""><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2010.5539970</idno>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010-06">2010. 2010</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,328.79,683.24,216.32,7.77;11,328.78,694.04,216.33,7.93;11,328.78,705.00,97.87,7.93" xml:id="b72">
	<monogr>
		<title level="m" type="main">Bridging the Gap between Few-Shot and Many-Shot Learning via Distribution Calibration</title>
		<author>
			<persName coords=""><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.36227/techrxiv.14380697</idno>
		<idno type="arXiv">arXiv:2101.06395</idno>
		<idno>2021. 1</idno>
		<imprint>
			<date type="published" when="2021-04-11" />
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,70.04,76.13,216.32,7.77;12,70.03,87.08,216.33,7.77;12,70.03,97.88,216.33,7.94;12,70.03,108.84,216.33,7.73;12,70.03,119.80,177.89,7.93" xml:id="b73">
	<analytic>
		<title level="a" type="main">DeepEMD: Few-Shot Image Classification With Differentiable Earth Mover‚Äôs Distance and Structured Classifiers</title>
		<author>
			<persName coords=""><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01222</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.04,131.92,216.32,7.77;12,70.03,142.87,216.33,7.77;12,70.03,153.67,216.33,7.93;12,70.03,164.63,129.17,7.93" xml:id="b74">
	<monogr>
		<title level="m" type="main">Collaboration of pre-trained models makes better few-shot learner</title>
		<author>
			<persName coords=""><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanqiu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.12255</idno>
		<idno>2022. 3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,70.04,176.75,216.32,7.77;12,70.03,187.71,216.33,7.77;12,70.03,198.67,216.33,7.77;12,70.03,209.46,216.33,7.93;12,70.03,220.58,49.32,7.77" xml:id="b75">
	<analytic>
		<title level="a" type="main">Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification</title>
		<author>
			<persName coords=""><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rongyao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19833-5_29</idno>
		<idno type="arXiv">arXiv:2111.03930</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
				<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="493" to="510" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,70.04,232.54,216.32,7.77;12,70.03,243.50,216.33,7.77;12,70.03,254.46,216.33,7.77;12,70.03,265.25,199.13,7.93" xml:id="b76">
	<monogr>
		<title level="m" type="main">Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training</title>
		<author>
			<persName coords=""><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rongyao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14401</idno>
		<idno>2022. 2</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,70.04,277.37,216.32,7.77;12,70.03,288.33,216.33,7.77;12,70.03,299.13,216.33,7.93;12,70.03,310.09,216.33,7.73;12,70.03,321.05,187.22,7.93" xml:id="b77">
	<analytic>
		<title level="a" type="main">PointCLIP: Point Cloud Understanding by CLIP</title>
		<author>
			<persName coords=""><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xupeng</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.00836</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">2022</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.04,333.16,216.32,7.77;12,70.03,344.12,216.33,7.77;12,70.03,354.92,216.33,7.93;12,70.03,366.04,4.48,7.77" xml:id="b78">
	<analytic>
		<title level="a" type="main">VinVL: Revisiting Visual Representations in Vision-Language Models</title>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr46437.2021.00553</idno>
		<idno type="arXiv">arXiv:2112.02399</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,70.04,377.99,216.32,7.77;12,70.03,388.95,216.33,7.77;12,70.03,399.75,216.33,7.93;12,70.03,410.71,138.13,7.93" xml:id="b79">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.06785</idno>
		<title level="m">Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,70.04,422.83,216.32,7.77;12,70.03,433.62,216.33,7.93;12,70.03,444.58,216.33,7.93;12,70.03,455.70,27.89,7.77" xml:id="b80">
	<analytic>
		<title level="a" type="main">Can Language Understand Depth?</title>
		<author>
			<persName coords=""><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziyao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yafeng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3503161.3549201</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
				<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-10-10">2022</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.04,467.66,216.32,7.77;12,70.03,478.62,216.33,7.77;12,70.03,489.57,22.42,7.77" xml:id="b81">
	<analytic>
		<title level="a" type="main">Conditional Prompt Learning for Vision-Language Models</title>
		<author>
			<persName coords=""><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.01631</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.04,501.53,216.32,7.77;12,70.03,512.33,216.33,7.93;12,70.03,523.29,129.17,7.93" xml:id="b82">
	<analytic>
		<title level="a" type="main">Conditional Prompt Learning for Vision-Language Models</title>
		<author>
			<persName coords=""><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.01631</idno>
		<idno type="arXiv">arXiv:2109.01134</idno>
		<idno>2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,70.04,535.40,216.32,7.77;12,70.03,546.36,216.33,7.77;12,70.03,557.16,216.33,7.93;12,70.03,568.12,216.33,7.93;12,70.03,579.24,73.46,7.77" xml:id="b83">
	<analytic>
		<title level="a" type="main">Conditional Prompt Learning for Vision-Language Models</title>
		<author>
			<persName coords=""><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.01631</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.04,591.19,216.32,7.77;12,70.03,602.15,216.33,7.77;12,70.03,612.95,216.33,7.93;12,70.03,623.91,97.87,7.93" xml:id="b84">
	<monogr>
		<title level="m" type="main">Pointclip v2: Adapting clip for powerful 3d open-world learning</title>
		<author>
			<persName coords=""><forename type="first">Xiangyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bowei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziyao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.11682</idno>
		<idno>2022. 3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
